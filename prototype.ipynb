{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380bf1ff",
   "metadata": {},
   "source": [
    "# Face Sonification Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6fdd0",
   "metadata": {},
   "source": [
    "ISSUES:\n",
    "* if you launch a synth (s_new), it's going to take some time until it is up and running: instantiation of a synth is asynchronous\n",
    "    * we may lose some messages before the synth is ready to play\n",
    "    * we should:\n",
    "        * start the synths in advance (maybe paused)\n",
    "        * pause synths when they are not useful instead of stopping them\n",
    "* often, we have a video recorded with a frame rate and sensor data that has been recorded with another frame rate\n",
    "    * PyQtGraph\n",
    "    * schedule visual events in the time queue\n",
    "* augmentation of the video with custom plotting, e.g. yourself plotting markers instead of having them plotted by OpenFace\n",
    "    * discuss if and how\n",
    "\n",
    "* The timing problem that OpenFace has seems to be related with OpenCV. The tracked video sometimes is sligtly shorter than the original one and sometimes it is longer. It seems that OpenCV has some problems working with some of the codecs.\n",
    "    * ffmpeg -i video.file -r 30 -vcodec ffv1 -acodec pcm_s16le output_name\n",
    "        * in my case, this does not work\n",
    "    * ffmpeg -i video.file -r 30 -vcodec ffv1 output_name\n",
    "    * split video frames into images if the previous solution fails.\n",
    "        * ffmpeg\n",
    "\n",
    "* We can consider the idea of focusing on the AUs that OpenFace detects with good precision and maybe cut off some of the others. We can find this information in the OpenFace paper.\n",
    "* The smile seems to be in many context the most important source of information. We can consider the idea of designing sonifications to put this element in the foreground.\n",
    "* The event-based sonification for how I implemented can be confusing for the listener: events are triggered only when the intensity of the AU goes over some thresholds (i.e. 1, 2, 3, 4). This way, the events are able to provide some information on the dinamic behaviour of the sound, but it also means that if an expression stays in the same range for a long time, we don't really have a continuous feedback, as the sound fades out quickly. Another problem is that events are also triggered when the intensity pass from a high range to a lower range; even if this provides some feedback on the dynamic behaviour, it is counterintuitive, because the user would expect the event to be associated to the activation of an expression.\n",
    "* We can try to use envelope parameters to handle the fall down\n",
    "* We can try a more hybrid approach. Some AUs will be sonified using an event-based approach that will omit information relative to the intensity, so that one event for expression will be triggered, e.g. one blink is sonified as one drop: we ignore the information on the intensity of the blink; we can do this for many other expression that are not our primary focus. The most important expressions (e.g. the smile), will be sonified somehow continuously, so that the listener would be able to perceive their stationary behaviour as well.\n",
    "\n",
    "* We could think of a musical sonification when AUs are associated to tracks that would play well together. When an AU is present, the relative track is played. AUs that are incompatible can be mapped to tracks that don's sound well together.\n",
    "    * Rithmical structured tracks\n",
    "    * Ambient\n",
    "* Sonify the movements of the head and of the eyes. We can sonify the degree of variation to the standard direction, so that when e.g. the head is in a standard position, no sound is played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e61022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:28:00.039678Z",
     "start_time": "2021-11-25T18:27:51.515380Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb9707",
   "metadata": {},
   "source": [
    "## OpenFace workaround\n",
    "It works, but...\n",
    "* we have a very long conversion phase\n",
    "* the feature extraction is much longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2140736",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir full\n",
    "!ffmpeg -i full.avi -f image2 full/video-frame%05d.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9597dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec -it openface build/bin/FeatureExtraction -out_dir files/processed -fdir files/full -aus -gaze -pose -tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83783b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 is the maximum quality\n",
    "!ffmpeg -y -i full.avi -vcodec mpeg4 -q:v 0 -max_muxing_queue_size 2048 full-rigth-codecs.avi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbedcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i full.avi -vcodec libx264 -preset slow -crf 22 -max_muxing_queue_size 2048 -an full-rigth-codecs.avi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4cff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i full.avi -vcodec libx264 -preset slow -crf 0 -max_muxing_queue_size 2048 -an full-rigth-codecs.avi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05abdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i full.avi -vcodec libx264 -preset veryslow -crf 0 -max_muxing_queue_size 2048 -an full-rigth-codecs.avi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054af7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i full.avi -vcodec mpeg4 -q:v 1 -max_muxing_queue_size 2048 -an full-rigth-codecs.avi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e5419",
   "metadata": {},
   "source": [
    "## Container setup and video processing\n",
    "\n",
    "The following section assumes that OpenFace was installed using docker.\n",
    "\n",
    "As first step, run the container with the following command:\n",
    "* `docker run -it --rm --name openface --mount type=bind,source=/home/michele/Desktop/Thesis/media/files,target=/home/openface-build/files algebr/openface:latest`\n",
    "* Substitute /home/michele/Desktop/Thesis/media/files with the mounting point\n",
    "* The current working directory must contain a directory **files/**\n",
    "    * This directory is bound with the --mount option to the files/ directory present in the docker container\n",
    "    * This directory is shared between the file system of the host and the one of the container\n",
    "\n",
    "Now we can launch the OpenFace executables (present in the container) from outside the container environment with a command similar to the following:\n",
    "* `docker exec -it openface build/bin/FeatureExtraction -out_dir files/processed -f files/video.avi`\n",
    "* From python (this notebook) we can call the same process and read the results\n",
    "* Create the directory files/processed in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742cbcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:01:41.759644Z",
     "start_time": "2021-11-16T17:00:08.512375Z"
    }
   },
   "outputs": [],
   "source": [
    "# name given to the container\n",
    "CONTAINER_NAME = 'openface'\n",
    "\n",
    "# base directory of the container\n",
    "CONTAINER_BASE_DIR = '/home/openface-build'\n",
    "# directory with executalbles in the container\n",
    "CONTAINER_BIN_DIR = os.path.join(CONTAINER_BASE_DIR, 'build/bin')\n",
    "\n",
    "FILE_DIR = '../media/files'\n",
    "OUT_DIR = os.path.join(FILE_DIR, 'processed')\n",
    "\n",
    "CONTAINER_FILE_DIR = os.path.join(CONTAINER_BASE_DIR, 'files')\n",
    "CONTAINER_OUT_DIR = os.path.join(CONTAINER_FILE_DIR, 'processed')\n",
    "\n",
    "CONTAINER_EXECUTABLE = os.path.join(CONTAINER_BIN_DIR, 'FeatureExtraction')\n",
    "\n",
    "def feature_extraction_offline(video_name):\n",
    "    \n",
    "    video_path = os.path.join(FILE_DIR, video_name)\n",
    "    \n",
    "    # the file must be in FILE_DIR\n",
    "    if not os.path.isfile(video_path):\n",
    "        raise FileNotFoundError(video_path)\n",
    "    \n",
    "    container_video_path = os.path.join(CONTAINER_FILE_DIR, video_name)\n",
    "    \n",
    "    command = [\n",
    "        'docker', 'exec', CONTAINER_NAME, CONTAINER_EXECUTABLE,\n",
    "        '-f', container_video_path,\n",
    "        '-out_dir', CONTAINER_OUT_DIR,\n",
    "        # features extracted\n",
    "        '-pose', '-gaze', '-aus',\n",
    "        # output tracked video\n",
    "        '-tracked'\n",
    "    ]\n",
    "    \n",
    "    # capture and combine stdout and stderr into one stream and set as text stream\n",
    "    proc = subprocess.Popen(command,stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "        \n",
    "    # poll process and show its output\n",
    "    while True:\n",
    "        output = proc.stdout.readline()\n",
    "        \n",
    "        if output:\n",
    "            print(output.strip())\n",
    "            \n",
    "        if proc.poll() is not None:\n",
    "            break\n",
    "    \n",
    "    return proc\n",
    "\n",
    "def feature_extraction_online(pipe='files/pipe'):\n",
    "    \n",
    "    command = [\n",
    "        'docker', 'exec', CONTAINER_NAME, CONTAINER_EXECUTABLE,\n",
    "        '-device', # use default device\n",
    "        '-pose', '-gaze', '-aus',\n",
    "        # '-tracked'\n",
    "        '-of', pipe\n",
    "    ]\n",
    "    \n",
    "    # capture and combine stdout and stderr into one stream and set as text stream\n",
    "    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)    \n",
    "\n",
    "    print('Starting real-time analysis...')\n",
    "    print('Open the pipe from the read side to start the feature stream')\n",
    "    \n",
    "    return proc\n",
    "\n",
    "def read_openface_csv(csv_path):\n",
    "    return pd.read_csv(csv_path, sep=r',\\s*', engine='python')\n",
    "\n",
    "def ffmpeg_convert(in_file, out_file):\n",
    "    \n",
    "    # this was the original command that appeared in the notebook\n",
    "    # ffmpeg -y -i \"files/processed/phone.avi\" -c:v libx264 -preset slow -crf 22 -pix_fmt yuv420p -c:a libvo_aacenc -b:a 128k \"files/phone-processed.mp4\"\n",
    "    command = [\n",
    "        'ffmpeg', '-y',\n",
    "        '-i', in_file,\n",
    "        '-c:v', 'libx264',\n",
    "        '-crf', '22',\n",
    "        '-pix_fmt', 'yuv420p',\n",
    "        '-c:a', 'libvo_aacenc',\n",
    "        '-b:a', '128k',\n",
    "        out_file\n",
    "    ]\n",
    "    \n",
    "    proc = subprocess.run(command, capture_output=True)\n",
    "    \n",
    "    return proc\n",
    "\n",
    "def ffmpeg_merge(video_file, audio_file, out_file):\n",
    "    \n",
    "    # ffmpeg -i files/phone-processed.mp4 -i score.wav  -c:v copy phone-processed-son.mp4 -y\n",
    "                \n",
    "    command = [\n",
    "        'ffmpeg', '-y',\n",
    "        '-i', video_file,\n",
    "        '-i', audio_file,\n",
    "        '-map', '0:v',\n",
    "        '-map', '1:a',\n",
    "        '-c:v', 'copy',\n",
    "        out_file\n",
    "    ]\n",
    "    \n",
    "    proc = subprocess.run(command, capture_output=True)\n",
    "    \n",
    "    return proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df9c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'full-rigth-codecs.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f356ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time feature_extraction_offline(video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_convert(\"files/processed/phone.avi\", \"files/phone-processed.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975b680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:56:47.210068Z",
     "start_time": "2021-11-26T00:56:46.558152Z"
    }
   },
   "outputs": [],
   "source": [
    "df = read_openface_csv(os.path.join(OUT_DIR, \"full.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c962c08",
   "metadata": {},
   "source": [
    "## Sonification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e803b",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547bd7f",
   "metadata": {},
   "source": [
    "Here we are using the current **develop** branch of sc3nb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fcec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:56:54.169321Z",
     "start_time": "2021-11-26T00:56:54.160802Z"
    }
   },
   "outputs": [],
   "source": [
    "import sc3nb as scn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e8ce4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:57:34.103220Z",
     "start_time": "2021-11-26T00:57:30.546086Z"
    }
   },
   "outputs": [],
   "source": [
    "# start scsynth\n",
    "sc = scn.startup()\n",
    "# connect scsynth to the system playback\n",
    "!jack_connect \"SuperCollider:out_1\" \"system:playback_1\"\n",
    "!jack_connect \"SuperCollider:out_2\" \"system:playback_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d8053",
   "metadata": {},
   "source": [
    "If this does not work, open QJackCtl and link the nodes in the graph manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d52a89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:57:06.929914Z",
     "start_time": "2021-11-26T00:57:06.499795Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373d782",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:57:56.377482Z",
     "start_time": "2021-11-26T00:57:56.355915Z"
    }
   },
   "outputs": [],
   "source": [
    "# test sound output\n",
    "sc.server.blip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf1d3c",
   "metadata": {},
   "source": [
    "Here we want to sonify Action Units (AUs).\n",
    "\n",
    "OpenFace is able to recognize a **subset** of AUs, specifically: 1, 2, 4, 5, 6, 7, 9, 10, 12, 14, 15, 17, 20, 23, 25, 26, 28, and 45.\n",
    "\n",
    "AUs can be described in two ways, both supported by OpenFace:\n",
    "* Presence - if AU is visible in the face\n",
    "    * 0/1 value\n",
    "    * e.g. AU01_c, for presence of AU 1\n",
    "* Intensity - how intense is the AU (minimal to maximal) on a 5 point scale\n",
    "    * \\[0, 5\\] values\n",
    "        * 0: not present\n",
    "        * 1: present at minimum intensity\n",
    "        * 5: present at maximum intensity\n",
    "    * e.g. AU01_r, for intensity of AU 1\n",
    "\n",
    "NOTE: intensity and presence predictors have been trained separately and on slightly different datasets, this means that **the predictions of both might not always be consistent** (e.g. the presence model could be predicting AU as not being present, but the intensity model could be predicting its value above 1).\n",
    "\n",
    "AU 28 (lip suck) has only the information regarding to the presence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202a0e6",
   "metadata": {},
   "source": [
    "In this prototype sonification, we will take into account only two AUs:\n",
    "* AU 4: Brow Lowerer\n",
    "* AU 28: Lip Suck\n",
    "    * Note: for this one OpenFace only provides information about the presence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad0a2e",
   "metadata": {},
   "source": [
    "### Non-Realtime\n",
    "\n",
    "* We can use NRT Synthesis: https://doc.sccode.org/Guides/Non-Realtime-Synthesis.html\n",
    "    * scsynth (in NRT mode) will process a score file given as an input (containing OSC commands) and generate an audio file.\n",
    "    * in our case we will generate the score file from the features extracted by OpenFace (CSV file).\n",
    "    * Commands that are considered asynchronous in realtime behave as synchronous commands in NRT.\n",
    "        * simply front-load your Score with all the SynthDefs and Buffers, at time 0.0, and then start the audio processing also at time 0.0\n",
    "            * you might need a slight offset for the audio processing because sort may not know which entries at time 0.0 must come first.\n",
    "* sc3nb allows you to do NRT synthesis through its Score class (interface to sclang's Score class).\n",
    "    * The Score.record_nrt class method provides an easy interface to generates an OSC score file and the relative audio file from a **dict** with **timings as keys** and **lists of OSCMessages as values**.\n",
    "    * bundler.messages() returns a dictionary in this format\n",
    "    * Score.record_nrt returns a subprocess.CompletedProcess object with the info relative to the completed scsynth NRT process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dee2ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T19:32:11.965592Z",
     "start_time": "2021-11-21T19:32:07.197712Z"
    }
   },
   "outputs": [],
   "source": [
    "from sc3nb import Score, SynthDef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751558d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T02:30:08.248823Z",
     "start_time": "2021-11-22T02:30:08.244209Z"
    }
   },
   "outputs": [],
   "source": [
    "# synthdef = SynthDef(\n",
    "#     \"test\",\n",
    "#     r\"\"\"{ |out, freq = 440|\n",
    "#             OffsetOut.ar(out,\n",
    "#                 SinOsc.ar(freq, 0, 0.2) * Line.kr(1, 0, 0.5, doneAction: Done.freeSelf)\n",
    "#             )\n",
    "#         }\"\"\",\n",
    "# )\n",
    "\n",
    "synthdef = SynthDef(\n",
    "    \"s2\",\n",
    "    r\"\"\"{ | freq=400, amp=0.3, num=4, pan=0, lg=0.1, gate=1 |\n",
    "            Out.ar(0, Pan2.ar(\n",
    "                Blip.ar(freq.lag(lg),  num) * EnvGen.kr(Env.asr(0.0, 1.0, 1.0), gate, doneAction: Done.freeSelf),\n",
    "                    pan.lag(lg),\n",
    "                    amp.lag(lg)\n",
    "                )\n",
    "            )\n",
    "        }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994c668",
   "metadata": {},
   "source": [
    "The recommended way to proceed it to build a Bundler and use it to generate score file and audio rendering instead of sending it to the server.\n",
    "\n",
    "* freq=400, amp=0.3, num=4, pan=0, lg=0.1, gate=1\n",
    "* /s_new:\n",
    "    * synth definition name\n",
    "    * synth ID\n",
    "    * add action (0,1,2, 3 or 4 see below)\n",
    "    * add target ID\n",
    "    * N*\t\n",
    "int or string\ta control index or name\n",
    "float or int or string\tfloating point and integer arguments are interpreted as control value. a symbol argument consisting of the letter 'c' or 'a' (for control or audio) followed by the bus's index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e10ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T03:04:47.238952Z",
     "start_time": "2021-11-22T03:04:47.055278Z"
    }
   },
   "outputs": [],
   "source": [
    "with sc.server.bundler(send_on_exit=False) as bundler:\n",
    "    # setup at the beginning of the score\n",
    "    synthdef.add()\n",
    "    \n",
    "    # instantiate synths\n",
    "    bundler.add(0.0, \"/s_new\", [\"s2\", au4_node_id, 0, 0, \"amp\", 0])\n",
    "    \n",
    "    # iterate over dataframe rows\n",
    "    for _, row in df.iterrows():\n",
    "        # print (row[\"timestamp\"], row[\"AU04_c\"], row[\"AU04_r\"], row[\"AU28_c\"])\n",
    "        \n",
    "        timestamp = row[\"timestamp\"]\n",
    "        intensity = row[\"AU04_r\"]\n",
    "        \n",
    "        # only \"max\" should be enough (to clip the top part to 0.3)\n",
    "        amp = scn.linlin(intensity, 0, 1, 0, 0.3, \"minmax\")   # TODO: exponential mapping\n",
    "        # map the intensity of the AU in one octave range\n",
    "        freq = scn.midicps(scn.linlin(intensity, 0, 5, 69, 81))\n",
    "        \n",
    "        # print(amp, freq)\n",
    "        bundler.add(timestamp, \"/n_set\", [au4_node_id, \"amp\", amp, 'freq', freq])\n",
    "\n",
    "    # assumption: the feature rate is constant\n",
    "    feature_interval = df.iloc[1]['timestamp']\n",
    "    # The /c_set [0, 0] will close the audio file\n",
    "    bundler.add(row[\"timestamp\"] + feature_interval, \"/c_set\", [0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89bb2a7",
   "metadata": {},
   "source": [
    "Generate the score file and render it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0993f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T03:04:53.100632Z",
     "start_time": "2021-11-22T03:04:52.662794Z"
    }
   },
   "outputs": [],
   "source": [
    "Score.record_nrt(bundler.messages(), \"/tmp/score.osc\", \"score.wav\", header_format=\"WAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16e757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T03:05:11.523818Z",
     "start_time": "2021-11-22T03:05:07.660382Z"
    }
   },
   "outputs": [],
   "source": [
    "ffmpeg_merge('files/phone-processed.mp4', 'score.wav', 'phone-processed-son.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0299d",
   "metadata": {},
   "source": [
    "### Real-time\n",
    "\n",
    "Real-time sonification can be used both for video streams recorded real-time from a device (e.g. webcam) or with video files. In this last case, the advantages with respect to NRT are that the user can start the sonification immediately, without having to wait for the whole video to be processed, and that the user could interactively change the parameters of the sonification on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afb4c7",
   "metadata": {},
   "source": [
    "#### Online usage\n",
    "\n",
    "When we track the webcam real-time (-device option), OpenFace (FeatureExtraction executable) opens the stream of the specified device and attempts to open a window with a real-time visualization of the extracted features.\n",
    "\n",
    "If we are using OpenFace from docker, we have to be careful of granting to the container the access to the webcam device and to the xserver (on Linux).\n",
    "* The **xhost** program is used to add and delete host names or user names to the list allowed to make connections to the X server\n",
    "    * `xhost +` grants access to everyone, even if they aren't on the list (i.e., access control is turned off)\n",
    "    * TODO is it a security problem?\n",
    "* `docker run -it --rm --name openface --mount type=bind,source=$(pwd)/files,target=/home/openface-build/files --device=/dev/video0:/dev/video0 --net=host -e DISPLAY=$DISPLAY algebr/openface:latest`\n",
    "    * `--device=/dev/video0:/dev/video0`\n",
    "    * `--net=host`\n",
    "    * `-e DISPLAY=$DISPLAY`\n",
    "\n",
    "If we launch Feature extraction with `-device /dev/video0` (or the right device URL), the executable will start to analyze the stream of the camera and start to generate the usual output files, of which we are particularly interested in the CSV file containing the extracted features. OpenFace appends the features to this file while analyzes the stream; after the stream is closed, OpenFace executes a post-processing on the CSV file.\n",
    "\n",
    "In this case, as we want the computation to be as light as possible, we will launch the executable with the following options: `-pose -gaze -aus`. This way, OpenFace will not generate anything else than the pose, gaze and AUs features (contained in the CSV file); will not generate e.g. the tracked video.\n",
    "\n",
    "In this case we would like OpenFace to stream those features directly to our python program, so that somehow our program could process them in real-time. These are some Github issues that are relevant for this topic:\n",
    "* https://github.com/TadasBaltrusaitis/OpenFace/issues/375\n",
    "* https://github.com/TadasBaltrusaitis/OpenFace/issues/409\n",
    "* https://github.com/TadasBaltrusaitis/OpenFace/issues/492\n",
    "* https://github.com/TadasBaltrusaitis/OpenFace/issues/546\n",
    "* https://github.com/TadasBaltrusaitis/OpenFace/issues/898\n",
    "\n",
    "Fundamentally, the possible solutions are 2:\n",
    "* Modify the C++ code of OpenFace, so that the real-time features are sent to our application using a **messaging library**\n",
    "    * A project do this using the ZeroMQ library, but it only works for windows\n",
    "    * Modifying the code of OpenFace may slow down the development a lot and it's probably very complex\n",
    "* Using named pipes (**FIFOs**): the CSV file would be written to a named pipe (a special file)\n",
    "    * Should be possible on both Windows and Unix, with some differences\n",
    "    * This solution is mentioned by the developer in the issue https://github.com/TadasBaltrusaitis/OpenFace/issues/409, but nothing more than this is said; nobody tested it. Will it be performant enough for a real-time application?\n",
    "    * This should be fast to implement\n",
    "    \n",
    "FeatureExtraction has a `-of` option to specify the base name of the output files. The base name will be appended with all the extensions (e.g. .csv) to generate the output files; this is the only way to specify a name for the output CSV file, even if indirectly.\n",
    "\n",
    "The following is a workaround\n",
    "* It would be much better if FeatureExtraction supported a `--pipe` option for explicitly supporting specifying a named pipe where to send the extracted features, independently from the other possibly generated files.\n",
    "* I opened an issue on Github here: https://github.com/TadasBaltrusaitis/OpenFace/issues/995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0ab12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:28:24.317432Z",
     "start_time": "2021-11-25T18:28:23.429763Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkfifo files/pipe.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79612d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T13:02:58.413028Z",
     "start_time": "2021-11-17T13:02:58.407503Z"
    }
   },
   "outputs": [],
   "source": [
    "# !rm files/pipe.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9986bdf",
   "metadata": {},
   "source": [
    "`docker exec -it openface build/bin/FeatureExtraction -device /dev/video0 -pose -gaze -aus -of files/pipe`\n",
    "* Launching this command will attempt to write the features in the file pipe.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_online()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d728b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T19:10:57.059348Z",
     "start_time": "2021-11-25T19:10:55.939748Z"
    }
   },
   "outputs": [],
   "source": [
    "# proc.kill() does not work with docker\n",
    "# we have to use FeatureExt... why?\n",
    "# !docker exec -it openface ps -el\n",
    "!docker exec -it openface pkill FeatureExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff589a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T17:25:57.043630Z",
     "start_time": "2021-11-24T17:25:30.925105Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "FIFO = os.path.join(FILE_DIR, 'pipe.csv')\n",
    "\n",
    "# send synth definitions to the server here?\n",
    "\n",
    "# instantiate synths\n",
    "init()\n",
    "\n",
    "# open FIFO for reading\n",
    "with open(FIFO, 'r') as fifo:\n",
    "    \n",
    "    # the reader attempts to execute fifo.readline() (which is blocking if there are no lines)\n",
    "    reader = csv.reader(fifo, skipinitialspace=True)\n",
    "    \n",
    "    # the first line written to the pipe is the CSV header\n",
    "    header = next(reader)\n",
    "    # transform into dictionary of indexes\n",
    "    header_dict = {label: idx for idx, label in enumerate(header)}\n",
    "    \n",
    "    # the loop ends when the pipe is closed from the writing side\n",
    "    for row in reader:\n",
    "        # print(row)\n",
    "        sonify(row, header_dict)\n",
    "        \n",
    "    sc.server.free_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929a628",
   "metadata": {},
   "source": [
    "#### Offline usage\n",
    "\n",
    "In this case what we want is to simulate real-time execution with a recorded (and preprocessed) video.\n",
    "\n",
    "Maybe in this case the **streaming simulation** approach would be the most natural (and maybe the best one). Here we would loop over each line of the dataframe and sleep the required amount of time. Every message would be sent using the updated sonification parameters (we need some sort of concurrent construct to ensure thread-safety)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893988b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:21:27.712877Z",
     "start_time": "2021-11-26T01:21:16.510942Z"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# recorder = scn.Recorder(path=\"sonification.wav\")\n",
    "# recorder.start()\n",
    "\n",
    "# instantiate synths\n",
    "sc.server.msg(\"/s_new\", [\"s2\", au4_node_id, 0, 0, \"amp\", 0])\n",
    "\n",
    "# iterate over dataframe rows\n",
    "for _, row in df.iterrows():\n",
    "    \n",
    "    # only \"max\" should be enough (to clip the top part to 0.3)\n",
    "    amp = scn.linlin(row[\"AU04_r\"], 0, 1, 0, 0.3, \"minmax\")   # TODO: exponential mapping\n",
    "    # map the intensity of the AU in one octave range\n",
    "    freq = scn.midicps(scn.linlin(row[\"AU04_r\"], 0, 5, 69, 81))\n",
    "\n",
    "    # the bundle will play at t0 + timestamp\n",
    "    with sc.server.bundler(t0 + row.timestamp) as bundler:\n",
    "        sc.server.msg(\"/n_set\", [au4_node_id, \"amp\", amp, \"freq\", freq], bundle=True)\n",
    "    \n",
    "    # sleep for the missing time\n",
    "    waiting_time = t0 + row.timestamp - time.time()\n",
    "    \n",
    "    if waiting_time > 0:\n",
    "        time.sleep(waiting_time)\n",
    "    \n",
    "sc.server.free_all()\n",
    "# recorder.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d5e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:03:14.039783Z",
     "start_time": "2021-11-26T01:03:13.868257Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.server.free_all();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67df98",
   "metadata": {},
   "source": [
    "## Sonification Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4697ae",
   "metadata": {},
   "source": [
    "AUs recognized by OpenFace:\n",
    "* 1: Inner Brow Raiser\n",
    "* 2: Outer Brow Raiser (unilateral)\n",
    "* 4: Brow Lowerer\n",
    "* 5: Upper Lid Raiser\n",
    "* 6: Cheek Raiser\n",
    "* 7: Lid Tightener\n",
    "* 9: Nose Wrinkler (usually goes along with 4 and 10)\n",
    "* 10: Upper Lip Raiser\n",
    "* 12: Lip Corner Puller\n",
    "* 14: Dimpler (fossetta)\n",
    "* 15: Lip Corner Depressor \n",
    "* 17: Chin Raiser\n",
    "* 20: Lip Stretcher\n",
    "* 23: Lip Tightener (kiss)\n",
    "* 25: Lips Part (relax Mentalis, antagonist of AU17)\n",
    "* 26: Jaw Drop (usually goes along with 25)\n",
    "* 28: Lip Suck (usually along with 26) - OpenFace provides only information about presence\n",
    "* 45: Blink\n",
    "\n",
    "Other AUs are related to eye or head movement. For these OpenFace provides information on gaze and head orientation.\n",
    "\n",
    "Groups of AUs are interpretable as emotions.\n",
    "\n",
    "Notes:\n",
    "* How to sonify AU02? Do we have side info?\n",
    "* All the expressions except for AU02 are symmetric. Asymmetric expression can maybe generate confusion in OpenFace. It is likely that we will have to set a confidence thrashold to discard some of the data (this is also true for cases where improper lighting yields bad predictions\n",
    "    * Am I missing something?\n",
    "    * Read free book here: https://imotions.com/blog/facial-action-coding-system/\n",
    "* Many of the expressions are antagonists: they can't happen at the same time\n",
    "    * Antagonists\n",
    "        * 5 - 7\n",
    "        * 12 - 14 - 15\n",
    "        * 17 - 25\n",
    "        * ...\n",
    "    * We could think of giving the same instrument to the same group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6580b",
   "metadata": {},
   "source": [
    "Blinking seems to be a discrete event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5760ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"files/processed/phone.csv\", sep=r',\\s*', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98bce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.AU45_c == 1].AU45_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e078271b",
   "metadata": {},
   "source": [
    "sc3-plugins\n",
    "* http://doc.sccode.org/Classes/CrossoverDistortion.html\n",
    "* http://doc.sccode.org/Classes/Crest.html\n",
    "* http://doc.sccode.org/Classes/SVF.html\n",
    "* http://doc.sccode.org/Classes/OteyPiano.html\n",
    "* http://doc.sccode.org/Classes/MdaPiano.html\n",
    "* http://doc.sccode.org/Classes/SineShaper.html\n",
    "* http://doc.sccode.org/Classes/TwoTube.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bb8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install quarks\n",
    "%sc Quarks.gui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e81cb5",
   "metadata": {},
   "source": [
    "* sInstruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380573d",
   "metadata": {},
   "source": [
    "### Pentatonic eyes + brows\n",
    "* 1: Inner Brow Raiser\n",
    "* 2: Outer Brow Raiser (unilateral)\n",
    "* 4: Brow Lowerer\n",
    "* 5: Upper Lid Raiser\n",
    "* 6: Cheek Raiser\n",
    "* 7: Lid Tightener"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7a6c2",
   "metadata": {},
   "source": [
    "#### With s2 (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AU01_NODE_ID = 1001\n",
    "AU02_NODE_ID = 1002\n",
    "AU04_NODE_ID = 1003\n",
    "AU05_NODE_ID = 1004\n",
    "AU06_NODE_ID = 1005\n",
    "AU07_NODE_ID = 1006\n",
    "\n",
    "BASE_TONE = 69\n",
    "PENTATONIC = [\n",
    "    scn.midicps(BASE_TONE),\n",
    "    scn.midicps(BASE_TONE + 3),\n",
    "    scn.midicps(BASE_TONE + 5),\n",
    "    scn.midicps(BASE_TONE + 7),\n",
    "    scn.midicps(BASE_TONE + 10),\n",
    "    scn.midicps(BASE_TONE + 12)\n",
    "]\n",
    "\n",
    "def init():\n",
    "    sc.server.msg(\"/s_new\", [\"s2\", AU01_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[5], \"num\", 1])\n",
    "    sc.server.msg(\"/s_new\", [\"s2\", AU02_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[4], \"num\", 1])\n",
    "    sc.server.msg(\"/s_new\", [\"s2\", AU04_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[3], \"num\", 1])\n",
    "    sc.server.msg(\"/s_new\", [\"s2\", AU05_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[2], \"num\", 1])\n",
    "    sc.server.msg(\"/s_new\", [\"s2\", AU06_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[0], \"num\", 1])\n",
    "    sc.server.msg(\"/s_new\", [\"s2\", AU07_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[1], \"num\", 1])\n",
    "    \n",
    "def map_intensity(intensity):\n",
    "    if intensity < 1.0:\n",
    "        amp = 0\n",
    "    else:\n",
    "        db = scn.linlin(intensity, 1, 5, -20, -5, \"minmax\")\n",
    "        amp = scn.dbamp(db)\n",
    "        \n",
    "    return amp\n",
    "\n",
    "def sonify(row, header_dict):\n",
    "    with sc.server.bundler() as bundler:\n",
    "        intensity = float(row[header_dict[\"AU01_r\"]])\n",
    "        amp = map_intensity(intensity)\n",
    "        sc.server.msg(\"/n_set\", [AU01_NODE_ID, \"amp\", amp], bundle=True)\n",
    "        \n",
    "        intensity = float(row[header_dict[\"AU02_r\"]])\n",
    "        amp = map_intensity(intensity)\n",
    "        sc.server.msg(\"/n_set\", [AU02_NODE_ID, \"amp\", amp], bundle=True)\n",
    "        \n",
    "        intensity = float(row[header_dict[\"AU04_r\"]])\n",
    "        amp = map_intensity(intensity)\n",
    "        sc.server.msg(\"/n_set\", [AU04_NODE_ID, \"amp\", amp], bundle=True)\n",
    "        \n",
    "        intensity = float(row[header_dict[\"AU05_r\"]])\n",
    "        amp = map_intensity(intensity)\n",
    "        sc.server.msg(\"/n_set\", [AU05_NODE_ID, \"amp\", amp], bundle=True)\n",
    "        \n",
    "        intensity = float(row[header_dict[\"AU06_r\"]])\n",
    "        amp = map_intensity(intensity)\n",
    "        sc.server.msg(\"/n_set\", [AU06_NODE_ID, \"amp\", amp], bundle=True)\n",
    "        \n",
    "        intensity = float(row[header_dict[\"AU07_r\"]])\n",
    "        amp = map_intensity(intensity)\n",
    "        sc.server.msg(\"/n_set\", [AU07_NODE_ID, \"amp\", amp], bundle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d42b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sc.server.bundler(send_on_exit=False) as bundler:\n",
    "    # setup at the beginning of the score\n",
    "    synthdef.add()\n",
    "    \n",
    "    # instantiate synths\n",
    "    bundler.add(0.0, \"/s_new\", [\"s2\", AU01_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[5], \"num\", 1])\n",
    "    bundler.add(0.0, \"/s_new\", [\"s2\", AU02_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[4], \"num\", 1])\n",
    "    bundler.add(0.0, \"/s_new\", [\"s2\", AU04_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[3], \"num\", 1])\n",
    "    bundler.add(0.0, \"/s_new\", [\"s2\", AU05_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[2], \"num\", 1])\n",
    "    bundler.add(0.0, \"/s_new\", [\"s2\", AU06_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[0], \"num\", 1])\n",
    "    bundler.add(0.0, \"/s_new\", [\"s2\", AU07_NODE_ID, 0, 0, \"amp\", 0, \"freq\", PENTATONIC[1], \"num\", 1])\n",
    "    \n",
    "    # iterate over dataframe rows\n",
    "    for _, row in df.iterrows():\n",
    "        # print (row[\"timestamp\"], row[\"AU04_c\"], row[\"AU04_r\"], row[\"AU28_c\"])\n",
    "        \n",
    "        timestamp = row[\"timestamp\"]\n",
    "        \n",
    "        intensity = row[\"AU01_r\"]\n",
    "        amp = map_intensity(intensity)\n",
    "        bundler.add(timestamp, \"/n_set\", [AU01_NODE_ID, \"amp\", amp])\n",
    "        \n",
    "        intensity = row[\"AU02_r\"]\n",
    "        amp = map_intensity(intensity)\n",
    "        bundler.add(timestamp, \"/n_set\", [AU02_NODE_ID, \"amp\", amp])\n",
    "        \n",
    "        intensity = row[\"AU04_r\"]\n",
    "        amp = map_intensity(intensity)\n",
    "        bundler.add(timestamp, \"/n_set\", [AU04_NODE_ID, \"amp\", amp])\n",
    "        \n",
    "        intensity = row[\"AU05_r\"]\n",
    "        amp = map_intensity(intensity)\n",
    "        bundler.add(timestamp, \"/n_set\", [AU05_NODE_ID, \"amp\", amp])\n",
    "        \n",
    "        intensity = row[\"AU06_r\"]\n",
    "        amp = map_intensity(intensity)\n",
    "        bundler.add(timestamp, \"/n_set\", [AU06_NODE_ID, \"amp\", amp])\n",
    "        \n",
    "        intensity = row[\"AU07_r\"]\n",
    "        amp = map_intensity(intensity)\n",
    "        bundler.add(timestamp, \"/n_set\", [AU07_NODE_ID, \"amp\", amp])\n",
    "    \n",
    "    bundler.add(row[\"timestamp\"], \"/c_set\", [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fa75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Score.record_nrt(bundler.messages(), \"/tmp/score.osc\", \"score.wav\", header_format=\"WAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_merge('files/pentatonic-test.avi', 'score.wav', 'pentatonic-test-son-no-processed.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fe3b6f",
   "metadata": {},
   "source": [
    "#### With MdaPiano (event based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212adf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc3nb import Score, SynthDef\n",
    "\n",
    "synthdef = SynthDef(\n",
    "    \"mdapiano\",\n",
    "    r\"\"\"{ |freq=440, gate=1, vel=100, amp=1|\n",
    "        var piano = MdaPiano.ar(\n",
    "            freq,\n",
    "            gate,\n",
    "            vel,\n",
    "            decay: 0,\n",
    "            release: 0,\n",
    "            hard: 0,\n",
    "            stereo: 0,\n",
    "            sustain: 0,\n",
    "            mul: amp\n",
    "        );\n",
    "        DetectSilence.ar(piano, 0.01, doneAction:2);\n",
    "        Out.ar(0, piano);\n",
    "    }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sc.server.bundler(send_on_exit=False) as bundler:\n",
    "    # setup at the beginning of the score\n",
    "    synthdef.add()\n",
    "    \n",
    "    old_range_level_01 = 0\n",
    "    cur_range_level_01 = 0\n",
    "    \n",
    "    old_range_level_02 = 0\n",
    "    cur_range_level_02 = 0\n",
    "    \n",
    "    old_range_level_04 = 0\n",
    "    cur_range_level_04 = 0\n",
    "    \n",
    "    old_range_level_05 = 0\n",
    "    cur_range_level_05 = 0\n",
    "    \n",
    "    old_range_level_06 = 0\n",
    "    cur_range_level_06 = 0\n",
    "    \n",
    "    old_range_level_07 = 0\n",
    "    cur_range_level_07 = 0\n",
    "    \n",
    "    # iterate over dataframe rows\n",
    "    for _, row in df.iterrows():\n",
    "        \n",
    "        timestamp = row[\"timestamp\"]\n",
    "        \n",
    "        intensity = row[\"AU01_r\"]\n",
    "        cur_range_level_01 = int(intensity)\n",
    "        \n",
    "        if cur_range_level_01 != old_range_level_01:\n",
    "            if cur_range_level_01 >= 1:\n",
    "                vel = scn.linlin(cur_range_level_01, 1, 5, 40, 127)\n",
    "                bundler.add(timestamp, \"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[5], \"vel\", vel])\n",
    "            old_range_level_01 = cur_range_level_01\n",
    "            \n",
    "        intensity = row[\"AU02_r\"]\n",
    "        cur_range_level_02 = int(intensity)\n",
    "        \n",
    "        if cur_range_level_02 != old_range_level_02:\n",
    "            if cur_range_level_02 >= 1:\n",
    "                vel = scn.linlin(cur_range_level_02, 1, 5, 40, 127)\n",
    "                bundler.add(timestamp, \"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[4], \"vel\", vel])\n",
    "            old_range_level_02 = cur_range_level_02\n",
    "            \n",
    "        intensity = row[\"AU04_r\"]\n",
    "        cur_range_level_04 = int(intensity)\n",
    "        \n",
    "        if cur_range_level_04 != old_range_level_04:\n",
    "            if cur_range_level_04 >= 1:\n",
    "                vel = scn.linlin(cur_range_level_04, 1, 5, 40, 127)\n",
    "                bundler.add(timestamp, \"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[3], \"vel\", vel])\n",
    "            old_range_level_04 = cur_range_level_04\n",
    "            \n",
    "        intensity = row[\"AU05_r\"]\n",
    "        cur_range_level_05 = int(intensity)\n",
    "        \n",
    "        if cur_range_level_05 != old_range_level_05:\n",
    "            if cur_range_level_05 >= 1:\n",
    "                vel = scn.linlin(cur_range_level_05, 1, 5, 40, 127)\n",
    "                bundler.add(timestamp, \"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[2], \"vel\", vel])\n",
    "            old_range_level_05 = cur_range_level_05\n",
    "            \n",
    "        intensity = row[\"AU06_r\"]\n",
    "        cur_range_level_06 = int(intensity)\n",
    "        \n",
    "        if cur_range_level_06 != old_range_level_06:\n",
    "            if cur_range_level_06 >= 1:\n",
    "                vel = scn.linlin(cur_range_level_06, 1, 5, 40, 127)\n",
    "                bundler.add(timestamp, \"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[0], \"vel\", vel])\n",
    "            old_range_level_06 = cur_range_level_06\n",
    "            \n",
    "        intensity = row[\"AU07_r\"]\n",
    "        cur_range_level_07 = int(intensity)\n",
    "        \n",
    "        if cur_range_level_07 != old_range_level_07:\n",
    "            if cur_range_level_07 >= 1:\n",
    "                vel = scn.linlin(cur_range_level_07, 1, 5, 40, 127)\n",
    "                bundler.add(timestamp, \"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[1], \"vel\", vel])\n",
    "            old_range_level_07 = cur_range_level_07\n",
    "    \n",
    "    bundler.add(row[\"timestamp\"], \"/c_set\", [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Score.record_nrt(bundler.messages(), \"/tmp/score.osc\", \"score.wav\", header_format=\"WAV\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeac802",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_merge('files/pentatonic-test.avi', 'score.wav', 'pentatonic-test-piano-son-no-processed.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a61636",
   "metadata": {},
   "source": [
    "TODO: adjust amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb435ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_range_level_01 = 0\n",
    "cur_range_level_01 = 0\n",
    "\n",
    "old_range_level_02 = 0\n",
    "cur_range_level_02 = 0\n",
    "\n",
    "old_range_level_04 = 0\n",
    "cur_range_level_04 = 0\n",
    "\n",
    "old_range_level_05 = 0\n",
    "cur_range_level_05 = 0\n",
    "\n",
    "old_range_level_06 = 0\n",
    "cur_range_level_06 = 0\n",
    "\n",
    "old_range_level_07 = 0\n",
    "cur_range_level_07 = 0\n",
    "\n",
    "def init():\n",
    "    synthdef.add()\n",
    "\n",
    "def sonify(row, header_dict):\n",
    "    \n",
    "    global old_range_level_01\n",
    "    global cur_range_level_01\n",
    "\n",
    "    global old_range_level_02\n",
    "    global cur_range_level_02\n",
    "\n",
    "    global old_range_level_04\n",
    "    global cur_range_level_04\n",
    "\n",
    "    global old_range_level_05\n",
    "    global cur_range_level_05\n",
    "\n",
    "    global old_range_level_06\n",
    "    global cur_range_level_06\n",
    "\n",
    "    global old_range_level_07\n",
    "    global cur_range_level_07\n",
    "\n",
    "    intensity = float(row[header_dict[\"AU01_r\"]])\n",
    "    cur_range_level_01 = int(intensity)\n",
    "\n",
    "    if cur_range_level_01 != old_range_level_01:\n",
    "        if cur_range_level_01 >= 1:\n",
    "            vel = scn.linlin(cur_range_level_01, 1, 5, 40, 127)\n",
    "            sc.server.msg(\"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[5], \"vel\", vel])\n",
    "        old_range_level_01 = cur_range_level_01\n",
    "\n",
    "    intensity = float(row[header_dict[\"AU02_r\"]])\n",
    "    cur_range_level_02 = int(intensity)\n",
    "\n",
    "    if cur_range_level_02 != old_range_level_02:\n",
    "        if cur_range_level_02 >= 1:\n",
    "            vel = scn.linlin(cur_range_level_02, 1, 5, 40, 127)\n",
    "            sc.server.msg(\"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[4], \"vel\", vel])\n",
    "        old_range_level_02 = cur_range_level_02\n",
    "\n",
    "    intensity = float(row[header_dict[\"AU04_r\"]])\n",
    "    cur_range_level_04 = int(intensity)\n",
    "\n",
    "    if cur_range_level_04 != old_range_level_04:\n",
    "        if cur_range_level_04 >= 1:\n",
    "            vel = scn.linlin(cur_range_level_04, 1, 5, 40, 127)\n",
    "            sc.server.msg(\"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[3], \"vel\", vel])\n",
    "        old_range_level_04 = cur_range_level_04\n",
    "\n",
    "    intensity = float(row[header_dict[\"AU05_r\"]])\n",
    "    cur_range_level_05 = int(intensity)\n",
    "\n",
    "    if cur_range_level_05 != old_range_level_05:\n",
    "        if cur_range_level_05 >= 1:\n",
    "            vel = scn.linlin(cur_range_level_05, 1, 5, 40, 127)\n",
    "            sc.server.msg(\"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[2], \"vel\", vel])\n",
    "        old_range_level_05 = cur_range_level_05\n",
    "\n",
    "    intensity = float(row[header_dict[\"AU06_r\"]])\n",
    "    cur_range_level_06 = int(intensity)\n",
    "\n",
    "    if cur_range_level_06 != old_range_level_06:\n",
    "        if cur_range_level_06 >= 1:\n",
    "            vel = scn.linlin(cur_range_level_06, 1, 5, 40, 127)\n",
    "            sc.server.msg(\"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[0], \"vel\", vel])\n",
    "        old_range_level_06 = cur_range_level_06\n",
    "\n",
    "    intensity = float(row[header_dict[\"AU07_r\"]])\n",
    "    cur_range_level_07 = int(intensity)\n",
    "\n",
    "    if cur_range_level_07 != old_range_level_07:\n",
    "        if cur_range_level_07 >= 1:\n",
    "            vel = scn.linlin(cur_range_level_07, 1, 5, 40, 127)\n",
    "            sc.server.msg(\"/s_new\", [\"mdapiano\", -1, 0, 0, \"freq\", PENTATONIC[1], \"vel\", vel])\n",
    "        old_range_level_07 = cur_range_level_07\n",
    "    \n",
    "def end():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400898f7",
   "metadata": {},
   "source": [
    "### Blink "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a440acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc3nb import Score, SynthDef\n",
    "\n",
    "synthdef = SynthDef(\n",
    "    \"drop\",\n",
    "    r\"\"\"{ | freq=600, dp=1200, amp=0.5, dur=0.1, pan=0 |\n",
    "        var sig, env, fch;\n",
    "        fch = XLine.kr(freq, freq+dp, dur);\n",
    "        sig = SinOsc.ar(fch);\n",
    "        env = EnvGen.kr(Env.perc(0.001, dur, curve: -4), 1.0, doneAction: 2);\n",
    "        Out.ar(0, Pan2.ar(sig, pan, env*amp))\n",
    "    }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sc.server.bundler(send_on_exit=False) as bundler:\n",
    "    # setup at the beginning of the score\n",
    "    synthdef.add()\n",
    "    \n",
    "    blinking = False\n",
    "    \n",
    "    # iterate over dataframe rows\n",
    "    for _, row in df.iterrows():\n",
    "        \n",
    "        timestamp = row[\"timestamp\"]\n",
    "        presence = row[\"AU45_c\"]\n",
    "        \n",
    "        if blinking:\n",
    "            if presence == 0:\n",
    "                blinking = False\n",
    "        else:\n",
    "            if presence == 1:\n",
    "                blinking = True\n",
    "                bundler.add(timestamp, \"/s_new\", [\"drop\", -1, 0, 0])\n",
    "    \n",
    "    bundler.add(row[\"timestamp\"], \"/c_set\", [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sc.server.bundler(send_on_exit=False) as bundler:\n",
    "    # setup at the beginning of the score\n",
    "    synthdef.add()\n",
    "    \n",
    "    blinking = False\n",
    "    \n",
    "    # iterate over dataframe rows\n",
    "    for _, row in df.iterrows():\n",
    "        \n",
    "        timestamp = row[\"timestamp\"]\n",
    "        intensity = row[\"AU45_r\"]\n",
    "        \n",
    "        if blinking:\n",
    "            if intensity < 1:\n",
    "                blinking = False\n",
    "        elif intensity >= 1:\n",
    "            blinking = True\n",
    "            bundler.add(timestamp, \"/s_new\", [\"drop\", -1, 0, 0])\n",
    "    \n",
    "    bundler.add(row[\"timestamp\"], \"/c_set\", [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "Score.record_nrt(bundler.messages(), \"/tmp/score.osc\", \"score.wav\", header_format=\"WAV\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b711f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_merge('files/aufnahme3_part_1.mp4', 'score.wav', 'blink-aufnahme3_part_1.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2e991",
   "metadata": {},
   "source": [
    "### Multi-percussion sound (event-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2847b11",
   "metadata": {},
   "source": [
    "Similar samples\n",
    "* snares\n",
    "<!--     * <audio controls src=\"samples/132417__sajmund__percussion-clave-like-hit.wav\"/> -->\n",
    "<!--     * <audio controls src=\"samples/277397__earmark-audio__efev1-percussion-snare.wav\"/> -->\n",
    "* high bells\n",
    "<!--     * <audio controls src=\"samples/339808__inspectorj__hand-bells-c-db-single.wav\"/> -->\n",
    "<!--     * <audio controls src=\"samples/360327__inspectorj__triangle-8-hard-hit-a.wav\"/> -->\n",
    "<!--     * <audio controls src=\"samples/411574__inspectorj__alto-gong-metal-hit-b-h6-xy.wav\"/> -->\n",
    "<!--     * <audio controls src=\"samples/387715__jagadamba__gong-percussion.wav\"/> -->\n",
    "<!--     * <audio controls src=\"samples/414563__pjcohen__agogo-bell-low-velocity11.wav\"/> -->\n",
    "<!--     * <audio controls src=\"samples/public domain/566514__ginijoyce__turning-objects-into-percussion-78.wav\"/> -->\n",
    "<!--     * <audio controls src=\"samples/public domain/566386__ginijoyce__turning-objects-into-percussion-17.wav\"/> -->\n",
    "* crash\n",
    "<!--     * <audio controls src=\"samples/public domain/209874__veiler__pff-chrash-14.wav\"/> -->\n",
    "* electronic\n",
    "<!--     * <audio controls src=\"samples/public domain/487662__phonosupf__electronic-percussion-5.wav\"/> -->\n",
    "<!--     * <audio controls src=\"samples/207919__altemark__space-snare.wav\"/> -->\n",
    "<!--     * <audio controls src=\"samples/577014__nezuai__cartoon-percussion-3.wav\"/> -->\n",
    "* drums\n",
    "<!--     * <audio controls src=\"samples/public domain/439825__twentytwentymusic__electronic-percussion-2.wav\"/> -->\n",
    "    * <audio controls src=\"samples/public domain/439828__twentytwentymusic__electronic-percussion-6.wav\"/>\n",
    "<!--     * <audio controls src=\"samples/public domain/439829__twentytwentymusic__electronic-percussion-5.wav\"/> -->\n",
    "* bass drum\n",
    "<!--     * <audio controls src=\"samples/138358__minorr__bass-drum-p.wav\"/> -->\n",
    "<!--     * <audio controls src=\"samples/234746__sonidotv__legno-10.wav\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2f509",
   "metadata": {},
   "source": [
    "Concepts:\n",
    "* Sounds that usually plays together should be distinguishable\n",
    "* Sounds that play more often should be less intrusive\n",
    "* One area should have sounds that are somehow related\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 1: Inner Brow Raiser - <audio controls src=\"samples/360327__inspectorj__triangle-8-hard-hit-a.wav\"/>\n",
    "* 2: Outer Brow Raiser (unilateral) - <audio controls src=\"samples/339808__inspectorj__hand-bells-c-db-single.wav\"/>\n",
    "* 4: Brow Lowerer - <audio controls src=\"samples/411574__inspectorj__alto-gong-metal-hit-b-h6-xy.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 5: Upper Lid Raiser - <audio controls src=\"samples/277397__earmark-audio__efev1-percussion-snare.wav\"/>\n",
    "* 6: Cheek Raiser - <audio controls src=\"samples/public domain/439829__twentytwentymusic__electronic-percussion-5.wav\"/>\n",
    "* 7: Lid Tightener - <audio controls src=\"samples/132417__sajmund__percussion-clave-like-hit.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 9: Nose Wrinkler (usually goes along with 4 and 10) - <audio controls src=\"samples/577014__nezuai__cartoon-percussion-3.wav\"/>\n",
    "* 10: Upper Lip Raiser - <audio controls src=\"samples/public domain/566386__ginijoyce__turning-objects-into-percussion-17.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 12: Lip Corner Puller - <audio controls src=\"samples/public domain/566514__ginijoyce__turning-objects-into-percussion-78.wav\"/>\n",
    "* 14: Dimpler - <audio controls src=\"samples/public domain/487662__phonosupf__electronic-percussion-5.wav\"/>\n",
    "* 15: Lip Corner Depressor - <audio controls src=\"samples/387715__jagadamba__gong-percussion.wav\"/>\n",
    "* 20: Lip Stretcher - <audio controls src=\"samples/414563__pjcohen__agogo-bell-low-velocity11.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 23: Lip Tightener - <audio controls src=\"samples/public domain/209874__veiler__pff-chrash-14.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 17: Chin Raiser - <audio controls src=\"samples/public domain/439825__twentytwentymusic__electronic-percussion-2.wav\"/>\n",
    "* 25: Lips Part (relax Mentalis, antagonist of AU17) - <audio controls src=\"samples/138358__minorr__bass-drum-p.wav\"/>\n",
    "* 26: Jaw Drop (usually goes along with 25) - <audio controls src=\"samples/234746__sonidotv__legno-10.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 28: Lip Suck (usually along with 26) - <audio controls src=\"samples/207919__altemark__space-snare.wav\"/>\n",
    "    * OpenFace only provides presence information\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 45: Blink (drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270cf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AU01_SAMPLE_PATH = \"samples/360327__inspectorj__triangle-8-hard-hit-a.wav\"\n",
    "AU02_SAMPLE_PATH = \"samples/339808__inspectorj__hand-bells-c-db-single.wav\"\n",
    "AU04_SAMPLE_PATH = \"samples/411574__inspectorj__alto-gong-metal-hit-b-h6-xy.wav\"\n",
    "AU05_SAMPLE_PATH = \"samples/277397__earmark-audio__efev1-percussion-snare.wav\"\n",
    "AU06_SAMPLE_PATH = \"samples/public domain/439829__twentytwentymusic__electronic-percussion-5.wav\"\n",
    "AU07_SAMPLE_PATH = \"samples/132417__sajmund__percussion-clave-like-hit.wav\"\n",
    "AU09_SAMPLE_PATH = \"samples/577014__nezuai__cartoon-percussion-3.wav\"\n",
    "AU10_SAMPLE_PATH = \"samples/public domain/566386__ginijoyce__turning-objects-into-percussion-17.wav\"\n",
    "AU12_SAMPLE_PATH = \"samples/public domain/566514__ginijoyce__turning-objects-into-percussion-78.wav\"\n",
    "AU14_SAMPLE_PATH = \"samples/public domain/487662__phonosupf__electronic-percussion-5.wav\"\n",
    "AU15_SAMPLE_PATH = \"samples/387715__jagadamba__gong-percussion.wav\"\n",
    "AU17_SAMPLE_PATH = \"samples/public domain/439825__twentytwentymusic__electronic-percussion-2.wav\"\n",
    "AU20_SAMPLE_PATH = \"samples/414563__pjcohen__agogo-bell-low-velocity11.wav\"\n",
    "AU23_SAMPLE_PATH = \"samples/public domain/209874__veiler__pff-chrash-14.wav\"\n",
    "AU25_SAMPLE_PATH = \"samples/138358__minorr__bass-drum-p.wav\"\n",
    "AU26_SAMPLE_PATH = \"samples/234746__sonidotv__legno-10.wav\"\n",
    "AU28_SAMPLE_PATH = \"samples/207919__altemark__space-snare.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4666430a",
   "metadata": {},
   "source": [
    "TODO: DetectSilence does not act smoothly. When the synth is freed, the wave is cut and we hear a \"toc\" sound. It would be better to use an envelope or a line to control the amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc3nb import Score, SynthDef\n",
    "\n",
    "drop_def = SynthDef(\n",
    "    \"drop\",\n",
    "    r\"\"\"{ | freq=600, dp=1200, amp=0.5, dur=0.1, pan=0 |\n",
    "        var sig, env, fch;\n",
    "        fch = XLine.kr(freq, freq+dp, dur);\n",
    "        sig = SinOsc.ar(fch);\n",
    "        env = EnvGen.kr(Env.perc(0.001, dur, curve: -4), 1.0, doneAction: 2);\n",
    "        Out.ar(0, Pan2.ar(sig, pan, env*amp))\n",
    "    }\"\"\"\n",
    ")\n",
    "\n",
    "# define custom playbuf synth that frees the synth when the buffer fades out\n",
    "playbuf1_def = SynthDef(\n",
    "    \"playbuf1\",\n",
    "    r\"\"\"{| out=0, bufnum=0, rate=1, amp=0.2, pan=0 |\n",
    "        var sig;\n",
    "        sig = PlayBuf.ar(1, bufnum, rate*BufRateScale.kr(bufnum), doneAction:2);\n",
    "        DetectSilence.ar(sig, 0.1, doneAction:2);\n",
    "        Out.ar(0, Pan2.ar(sig, pan, amp));\n",
    "    }\"\"\"\n",
    ")\n",
    "\n",
    "# Number of channels that the buffer will be. This must be a fixed integer. The architecture of the SynthDef cannot change after it is compiled.\n",
    "playbuf2_def = SynthDef(\n",
    "    \"playbuf2\",\n",
    "    r\"\"\"{| out=0, bufnum=0, rate=1, amp=0.2, pan=0 |\n",
    "        var sig;\n",
    "        sig = PlayBuf.ar(2, bufnum, rate*BufRateScale.kr(bufnum), doneAction:2);\n",
    "        DetectSilence.ar(sig, 0.1, doneAction:2);\n",
    "        Out.ar(0, Pan2.ar(sig, pan, amp));\n",
    "    }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_range(timestamp, old_range_level, cur_range_level, bundler, buf):\n",
    "\n",
    "    if cur_range_level != old_range_level:\n",
    "        if cur_range_level >= 1:\n",
    "            db = scn.linlin(cur_range_level, 1, 5, -20, 0, \"minmax\")\n",
    "            amp = scn.dbamp(db)\n",
    "            \n",
    "            if buf.channels == 1:\n",
    "                synth = \"playbuf1\"\n",
    "            else:\n",
    "                synth = \"playbuf2\"\n",
    "            \n",
    "            bundler.add(timestamp, \"/s_new\", [synth, -1, 0, 0, \"bufnum\", buf.bufnum, \"amp\", amp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc3nb import Buffer\n",
    "\n",
    "with sc.server.bundler(send_on_exit=False) as bundler:\n",
    "    # synth defs\n",
    "    drop_def.add()\n",
    "    playbuf1_def.add()\n",
    "    playbuf2_def.add()\n",
    "    \n",
    "    buf01 = Buffer().read(AU01_SAMPLE_PATH)\n",
    "    buf02 = Buffer().read(AU02_SAMPLE_PATH)\n",
    "    buf04 = Buffer().read(AU04_SAMPLE_PATH)\n",
    "    buf05 = Buffer().read(AU05_SAMPLE_PATH)\n",
    "    buf06 = Buffer().read(AU06_SAMPLE_PATH)\n",
    "    buf07 = Buffer().read(AU07_SAMPLE_PATH)\n",
    "    buf09 = Buffer().read(AU09_SAMPLE_PATH)\n",
    "    buf10 = Buffer().read(AU10_SAMPLE_PATH)\n",
    "    buf12 = Buffer().read(AU12_SAMPLE_PATH)\n",
    "    buf14 = Buffer().read(AU14_SAMPLE_PATH)\n",
    "    buf15 = Buffer().read(AU15_SAMPLE_PATH)\n",
    "    buf17 = Buffer().read(AU17_SAMPLE_PATH)\n",
    "    buf20 = Buffer().read(AU20_SAMPLE_PATH)\n",
    "    buf23 = Buffer().read(AU23_SAMPLE_PATH)\n",
    "    buf25 = Buffer().read(AU25_SAMPLE_PATH)\n",
    "    buf26 = Buffer().read(AU26_SAMPLE_PATH)\n",
    "    buf28 = Buffer().read(AU28_SAMPLE_PATH)\n",
    "    \n",
    "    old_range_level_01 = cur_range_level_01 = 0\n",
    "    old_range_level_02 = cur_range_level_02 = 0\n",
    "    old_range_level_04 = cur_range_level_04 = 0\n",
    "    old_range_level_05 = cur_range_level_05 = 0\n",
    "    old_range_level_06 = cur_range_level_06 = 0\n",
    "    old_range_level_07 = cur_range_level_07 = 0\n",
    "    old_range_level_09 = cur_range_level_09 = 0\n",
    "    old_range_level_10 = cur_range_level_10 = 0\n",
    "    old_range_level_12 = cur_range_level_12 = 0\n",
    "    old_range_level_14 = cur_range_level_14 = 0\n",
    "    old_range_level_15 = cur_range_level_15 = 0\n",
    "    old_range_level_17 = cur_range_level_17 = 0\n",
    "    old_range_level_20 = cur_range_level_20 = 0\n",
    "    old_range_level_23 = cur_range_level_23 = 0\n",
    "    old_range_level_25 = cur_range_level_25 = 0\n",
    "    old_range_level_26 = cur_range_level_26 = 0\n",
    "    \n",
    "    au28_is_on = False\n",
    "    blinking = False\n",
    "    \n",
    "    # iterate over dataframe rows\n",
    "    for _, row in df.iterrows():\n",
    "        \n",
    "        timestamp = row[\"timestamp\"]\n",
    "        \n",
    "        cur_range_level_01 = int(row[\"AU01_r\"])\n",
    "        map_range(timestamp, old_range_level_01, cur_range_level_01, bundler, buf01)\n",
    "        old_range_level_01 = cur_range_level_01\n",
    "        \n",
    "        cur_range_level_02 = int(row[\"AU02_r\"])\n",
    "        map_range(timestamp, old_range_level_02, cur_range_level_02, bundler, buf02)\n",
    "        old_range_level_02 = cur_range_level_02\n",
    "        \n",
    "        cur_range_level_04 = int(row[\"AU04_r\"])\n",
    "        map_range(timestamp, old_range_level_04, cur_range_level_04, bundler, buf04)\n",
    "        old_range_level_04 = cur_range_level_04\n",
    "        \n",
    "        cur_range_level_05 = int(row[\"AU05_r\"])\n",
    "        map_range(timestamp, old_range_level_05, cur_range_level_05, bundler, buf05)\n",
    "        old_range_level_05 = cur_range_level_05\n",
    "        \n",
    "        cur_range_level_06 = int(row[\"AU06_r\"])\n",
    "        map_range(timestamp, old_range_level_06, cur_range_level_06, bundler, buf06)\n",
    "        old_range_level_06 = cur_range_level_06\n",
    "        \n",
    "        cur_range_level_07 = int(row[\"AU07_r\"])\n",
    "        map_range(timestamp, old_range_level_07, cur_range_level_07, bundler, buf07)\n",
    "        old_range_level_07 = cur_range_level_07\n",
    "        \n",
    "        cur_range_level_09 = int(row[\"AU09_r\"])\n",
    "        map_range(timestamp, old_range_level_09, cur_range_level_09, bundler, buf09)\n",
    "        old_range_level_09 = cur_range_level_09\n",
    "        \n",
    "        cur_range_level_10 = int(row[\"AU10_r\"])\n",
    "        map_range(timestamp, old_range_level_10, cur_range_level_10, bundler, buf10)\n",
    "        old_range_level_10 = cur_range_level_10\n",
    "        \n",
    "        cur_range_level_12 = int(row[\"AU12_r\"])\n",
    "        map_range(timestamp, old_range_level_12, cur_range_level_12, bundler, buf12)\n",
    "        old_range_level_12 = cur_range_level_12\n",
    "        \n",
    "        cur_range_level_14 = int(row[\"AU14_r\"])\n",
    "        map_range(timestamp, old_range_level_14, cur_range_level_14, bundler, buf14)\n",
    "        old_range_level_14 = cur_range_level_14\n",
    "        \n",
    "        cur_range_level_15 = int(row[\"AU15_r\"])\n",
    "        map_range(timestamp, old_range_level_15, cur_range_level_15, bundler, buf15)\n",
    "        old_range_level_15 = cur_range_level_15\n",
    "        \n",
    "        cur_range_level_17 = int(row[\"AU17_r\"])\n",
    "        map_range(timestamp, old_range_level_17, cur_range_level_17, bundler, buf17)\n",
    "        old_range_level_17 = cur_range_level_17        \n",
    "        \n",
    "        cur_range_level_20 = int(row[\"AU20_r\"])\n",
    "        map_range(timestamp, old_range_level_20, cur_range_level_20, bundler, buf20)\n",
    "        old_range_level_20 = cur_range_level_20\n",
    "        \n",
    "        cur_range_level_23 = int(row[\"AU23_r\"])\n",
    "        map_range(timestamp, old_range_level_23, cur_range_level_23, bundler, buf23)\n",
    "        old_range_level_23 = cur_range_level_23\n",
    "        \n",
    "        cur_range_level_25 = int(row[\"AU25_r\"])\n",
    "        map_range(timestamp, old_range_level_25, cur_range_level_25, bundler, buf25)\n",
    "        old_range_level_25 = cur_range_level_25\n",
    "        \n",
    "        cur_range_level_26 = int(row[\"AU26_r\"])\n",
    "        map_range(timestamp, old_range_level_26, cur_range_level_26, bundler, buf26)\n",
    "        old_range_level_26 = cur_range_level_26\n",
    "        \n",
    "#         presence = row[\"AU45_c\"]\n",
    "        \n",
    "#         if au28_is_on:\n",
    "#             if presence == 0:\n",
    "#                 au28_is_on = False\n",
    "#         elif presence == 1:\n",
    "#             au28_is_on = True            \n",
    "#             bundler.add(timestamp, \"/s_new\", [\"playbuf2\", -1, 0, 0, \"bufnum\", buf28.bufnum, \"amp\", 0.4])\n",
    "        \n",
    "        intensity = row[\"AU45_r\"]\n",
    "        \n",
    "        if blinking:\n",
    "            if intensity < 1:\n",
    "                blinking = False\n",
    "        elif intensity >= 1:\n",
    "            blinking = True\n",
    "            bundler.add(timestamp, \"/s_new\", [\"drop\", -1, 0, 0])\n",
    "\n",
    "    bundler.add(row[\"timestamp\"], \"/c_set\", [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Score.record_nrt(bundler.messages(), \"/tmp/score.osc\", \"score.wav\", header_format=\"WAV\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbe650",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_merge(os.path.join(OUT_DIR, \"full.avi\"), 'score.wav', '../media/nrt-renderings/full-processed.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7b1b3",
   "metadata": {},
   "source": [
    "### Harmonic emoitions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:face]",
   "language": "python",
   "name": "conda-env-face-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
