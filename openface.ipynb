{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380bf1ff",
   "metadata": {},
   "source": [
    "# OpenFace Sonification with Panson\n",
    "\n",
    "This notebook will introduce **panson**: a framework for interactive sonification based on **sc3nb**.\n",
    "\n",
    "Connecting panson with OpenFace, we will implement several examples of sonification of facial expressions, head rotations and gaze, while showing some of the potentialities of panson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e61022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:28:00.039678Z",
     "start_time": "2021-11-25T18:27:51.515380Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "import panson as ps\n",
    "from panson import bundle\n",
    "\n",
    "from math import pi, exp, log2\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6adbf",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e5419",
   "metadata": {},
   "source": [
    "### OpenFace docker container\n",
    "\n",
    "The following section assumes that OpenFace was installed using **docker**.\n",
    "\n",
    "**Run the container** with the following command:\n",
    "\n",
    "`docker run -it --rm --name openface --mount type=bind,source=/mount/dir,target=/home/openface-build/files algebr/openface:latest`\n",
    "\n",
    "Substitute /mount/dir with the absolute path of the mount point. This directory is where you need to put the video files to make sure that the OpenFace executable can access them. This is because the directory is shared between the file system of the host and the one of the container.\n",
    "\n",
    "Now we can launch OpenFace executables (present in the container) from outside the container with a command similar to the following:\n",
    "\n",
    "`docker exec -it openface build/bin/FeatureExtraction -out_dir files/processed -f files/video.avi`\n",
    "\n",
    "The output directory files/processed must be created in advance.\n",
    "\n",
    "Later in the notebook, python functions are provided to run the executables of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094ff92",
   "metadata": {},
   "source": [
    "When called in live mode (e.g. while perform feature extraction from a webcam), the executables will attempt to display a window, and fail with an error if not possible.\n",
    "\n",
    "If your sistem is running the X server as windowing system, the following cell will allow all programs to make connections with the X server. This will allow the container to display windows, but it is a workaround. It is not safe in the general case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!xhost +"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb772d",
   "metadata": {},
   "source": [
    "The following functions are used to interact with OpenFace executables.\n",
    "\n",
    "**feature_extraction_online** requires some explanation. OpenFace does not provide any supported way of streaming live features: when the executable is started in real-time mode, all it will do is write the features to an output .csv file. In the original plan of development of the project, a feature streaming server should have been implemented, making the executables capable of streaming features through the network. This was never implemented though.\n",
    "\n",
    "The workaround found is to make the executable write to a named pipe rather than to an ordinary file. Panson will be instructed to read data from the named pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742cbcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:01:41.759644Z",
     "start_time": "2021-11-16T17:00:08.512375Z"
    }
   },
   "outputs": [],
   "source": [
    "# name given to the container\n",
    "CONTAINER_NAME = 'openface'\n",
    "\n",
    "# base directory of the container\n",
    "CONTAINER_BASE_DIR = '/home/openface-build'\n",
    "# directory with executalbles in the container\n",
    "CONTAINER_BIN_DIR = os.path.join(CONTAINER_BASE_DIR, 'build/bin')\n",
    "\n",
    "FILE_DIR = '../media/files'\n",
    "OUT_DIR = os.path.join(FILE_DIR, 'processed')\n",
    "\n",
    "CONTAINER_FILE_DIR = os.path.join(CONTAINER_BASE_DIR, 'files')\n",
    "CONTAINER_OUT_DIR = os.path.join(CONTAINER_FILE_DIR, 'processed')\n",
    "\n",
    "CONTAINER_EXECUTABLE = os.path.join(CONTAINER_BIN_DIR, 'FeatureExtraction')\n",
    "\n",
    "\n",
    "def feature_extraction_offline(video_name):\n",
    "    \"\"\"Perform feature extraction on video file.\"\"\"\n",
    "    \n",
    "    video_path = os.path.join(FILE_DIR, video_name)\n",
    "    \n",
    "    # the file must be in FILE_DIR\n",
    "    if not os.path.isfile(video_path):\n",
    "        raise FileNotFoundError(video_path)\n",
    "    \n",
    "    container_video_path = os.path.join(CONTAINER_FILE_DIR, video_name)\n",
    "    \n",
    "    command = [\n",
    "        'docker', 'exec', CONTAINER_NAME, CONTAINER_EXECUTABLE,\n",
    "        '-f', container_video_path,\n",
    "        '-out_dir', CONTAINER_OUT_DIR,\n",
    "        # features extracted\n",
    "        '-pose', '-gaze', '-aus',\n",
    "        # output tracked video\n",
    "        '-tracked'\n",
    "    ]\n",
    "    \n",
    "    # capture and combine stdout and stderr into one stream and set as text stream\n",
    "    proc = subprocess.Popen(command,stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "        \n",
    "    # poll process and show its output\n",
    "    while True:\n",
    "        output = proc.stdout.readline()\n",
    "        \n",
    "        if output:\n",
    "            print(output.strip())\n",
    "            \n",
    "        if proc.poll() is not None:\n",
    "            break\n",
    "    \n",
    "    return proc\n",
    "\n",
    "def feature_extraction_online(pipe='files/pipe'):\n",
    "    \"\"\"Start online feature extraction and return.\"\"\"\n",
    "    \n",
    "    command = [\n",
    "        'docker', 'exec', CONTAINER_NAME, CONTAINER_EXECUTABLE,\n",
    "        '-device', '0', # use default device\n",
    "        '-pose', '-gaze', '-aus',\n",
    "        # '-tracked'\n",
    "        '-of', pipe\n",
    "    ]\n",
    "    \n",
    "    # capture and combine stdout and stderr into one stream and set as text stream\n",
    "    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)    \n",
    "\n",
    "    print('Starting real-time analysis...')\n",
    "    print('Open the pipe from the read side to start the feature stream')\n",
    "    \n",
    "    return proc\n",
    "\n",
    "def kill_feature_extraction_online():\n",
    "    \"\"\"Kill online feature extraction.\"\"\"\n",
    "    \n",
    "    # !docker exec -it openface pkill FeatureExt\n",
    "    command = ['docker', 'exec', CONTAINER_NAME, 'pkill', 'FeatureExt']\n",
    "    subprocess.run(command)\n",
    "\n",
    "def read_openface_csv(csv_path):\n",
    "    \"\"\"Read csv files produced by openface executables.\"\"\"\n",
    "    \n",
    "    return pd.read_csv(csv_path, sep=r',\\s*', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b725b",
   "metadata": {},
   "source": [
    "When executing **offline feature extraction**, sometimes OpenFace will drop some frames without processing them, causing the output data (and tracked video output) to be inaccurate. This may be related to OpenCV not being able to work with some of the codecs, but in general this is an open issue of OpenFace.\n",
    "\n",
    "One workaround is to:\n",
    "1. Split video frames into separate images\n",
    "    * `ffmpeg -i video.avi video_dir/frame%04d.jpg`\n",
    "2. Instruct OpenFace to process the frame directory with the **-fdir** option\n",
    "    * The csv output of openface will not contain timestamp information, so we would have to recreate them from frame files names\n",
    "    \n",
    "This notebook will mainly focus on real-time processing, as that's the most interesting part as concerns the framework. For this reason we will not use this workaround for offline processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0583e4",
   "metadata": {},
   "source": [
    "The following functions are used to perform operations on video files using **ffmpeg**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e02d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffmpeg_convert(in_file, out_file):\n",
    "    \"\"\"Convert video file from one format to the other.\"\"\"\n",
    "\n",
    "    command = [\n",
    "        'ffmpeg', '-y',\n",
    "        '-i', in_file,\n",
    "        '-c:v', 'libx264',\n",
    "        '-crf', '22',\n",
    "        '-pix_fmt', 'yuv420p',\n",
    "        '-c:a', 'libvo_aacenc',\n",
    "        '-b:a', '128k',\n",
    "        out_file\n",
    "    ]\n",
    "    \n",
    "    proc = subprocess.run(command, capture_output=True)\n",
    "    \n",
    "    return proc\n",
    "\n",
    "def ffmpeg_merge(video_file, audio_file, out_file):\n",
    "    \"\"\"Merge audio and video files.\"\"\"\n",
    "    \n",
    "    # ffmpeg -i files/phone-processed.mp4 -i score.wav  -c:v copy phone-processed-son.mp4 -y\n",
    "                \n",
    "    command = [\n",
    "        'ffmpeg', '-y',\n",
    "        '-i', video_file,\n",
    "        '-i', audio_file,\n",
    "        '-map', '0:v',\n",
    "        '-map', '1:a',\n",
    "        '-c:v', 'copy',\n",
    "        out_file\n",
    "    ]\n",
    "    \n",
    "    proc = subprocess.run(command, capture_output=True)\n",
    "    \n",
    "    return proc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b9f23",
   "metadata": {},
   "source": [
    "### Supercollider / sc3nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fcec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:56:54.169321Z",
     "start_time": "2021-11-26T00:56:54.160802Z"
    }
   },
   "outputs": [],
   "source": [
    "import sc3nb as scn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e8ce4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:57:34.103220Z",
     "start_time": "2021-11-26T00:57:30.546086Z"
    }
   },
   "outputs": [],
   "source": [
    "# start scsynth\n",
    "sc = scn.startup()\n",
    "\n",
    "# connect scsynth to the system playback\n",
    "!jack_connect \"SuperCollider:out_1\" \"system:playback_1\"\n",
    "!jack_connect \"SuperCollider:out_2\" \"system:playback_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d8053",
   "metadata": {},
   "source": [
    "If **jack_connect** is not available on your sistem, you can use other programmes to connect SuperCollider to the system output. For instance, with **QJackCtl** you can graphically link the nodes to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d52a89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:57:06.929914Z",
     "start_time": "2021-11-26T00:57:06.499795Z"
    }
   },
   "outputs": [],
   "source": [
    "# sc.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323ec32",
   "metadata": {},
   "source": [
    "Test SuperCollider output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373d782",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:57:56.377482Z",
     "start_time": "2021-11-26T00:57:56.355915Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.server.blip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0b7b4",
   "metadata": {},
   "source": [
    "Set latency of the server. This step is mandatory; the value should be appropriate with respect to the user's configuration and messages sent by the framework should be received on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.server.latency = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4dcfcc",
   "metadata": {},
   "source": [
    "## Panson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bfe07c",
   "metadata": {},
   "source": [
    "### Offline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd09035",
   "metadata": {},
   "source": [
    "In this way it is possible to load the feature extraction data produced by OpenFace executables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path is relative to the docker container\n",
    "csv_file = \"/path/to/file.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_openface_csv(os.path.join(OUT_DIR, csv_file))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c16e0b",
   "metadata": {},
   "source": [
    "### Online"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224cdab",
   "metadata": {},
   "source": [
    "When working with real-time data we will need the object declared hereafter. It is a **stream** object that allow the panson to get data from OpenFace executables (in real-time).\n",
    "\n",
    "FIFO_PATH is the path of the named pipe where OpenFace is writing its live feature extraction data. **feature_extraction_online** (added as an opening hook) writes by default to **files/pipe.csv**.\n",
    "\n",
    "The test method tries to get some data from the stream and returns some information about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39086593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from panson import streams\n",
    "\n",
    "FIFO_PATH = os.path.join(FILE_DIR, 'pipe.csv')\n",
    "\n",
    "openface_stream = streams.CsvFifo('openface', args=(FIFO_PATH,))\\\n",
    "        .add_open_hook(feature_extraction_online)\\\n",
    "        .add_close_hook(kill_feature_extraction_online)\\\n",
    "        .test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc4c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openface_stream.sample_size, openface_stream.dtype, openface_stream.fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbd88a",
   "metadata": {},
   "source": [
    "### Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0b7d9",
   "metadata": {},
   "source": [
    "#### Sonification\n",
    "\n",
    "Hereafter, there are some implementations in different coding styles of a continuous dummy sonification of AU 4. The intensity of the AU is simply mapped to the frequency of a continuous synth (we use the default synth s2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c973906",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_PATH = os.path.join(scn.resources.__path__[0], \"synthdefs\", \"s2.scsyndef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31cce1",
   "metadata": {},
   "source": [
    "It is possible to access the server instance of the sonification through the attribute **s**. This will be the default server most of the time.\n",
    "\n",
    "As the we don't want the sonification to have parameters, we will neglect the **init_parameters** method. Look later on in this notebook for examples of its usage.\n",
    "\n",
    "In **init_server** we load the server resources, such as synthdefs and buffers. Even if default synthdef may be loaded by default at server startup, we need to load it anyway explicitly to make NRT sonification (export) work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05b2fd",
   "metadata": {},
   "source": [
    "#### Message-style (explicit ID allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc44d2c",
   "metadata": {},
   "source": [
    "##### Explicit bundling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc3nb.osc.osc_communication import Bundler\n",
    "\n",
    "# message style (explicit ID allocation)\n",
    "class AU04ContinuousSonification(ps.Sonification):\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        pass\n",
    "    \n",
    "    def init_server(self):\n",
    "        bundler = Bundler()\n",
    "        bundler.add(0, \"/d_load\", [S2_PATH])\n",
    "        return bundler\n",
    "    \n",
    "    def start(self):\n",
    "        bundler = Bundler()\n",
    "        self.au4_node_id = self.s.node_ids.allocate(1)[0]        \n",
    "        bundler.add(0, \"/s_new\", [\"s2\", self.au4_node_id, 0, 0, \"amp\", 0])\n",
    "        return bundler\n",
    "    \n",
    "    def stop(self, server):\n",
    "        bundler = Bundler()\n",
    "        self.s.node_ids.free([self.au4_node_id])    # actually this does nothing\n",
    "        bundler.add(0, \"/g_freeAll\", [0])\n",
    "        return bundler\n",
    "    \n",
    "    def _process(self, row):\n",
    "        bundler = Bundler()\n",
    "        # only \"max\" should be enough (to clip the top part to 0.3)\n",
    "        amp = scn.linlin(row[\"AU04_r\"], 0, 1, 0, 0.3, \"minmax\")\n",
    "        # map the intensity of the AU in one octave range\n",
    "        freq = scn.midicps(scn.linlin(row[\"AU04_r\"], 0, 5, 69, 81))\n",
    "        bundler.add(0, \"/n_set\", [self.au4_node_id, \"amp\", amp, \"freq\", freq])\n",
    "        return bundler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbcd00",
   "metadata": {},
   "source": [
    "##### Implicit bundling\n",
    "\n",
    "The **bundle decorator** can be used to capture the messages produced by the methods and return them in a bundler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AU04ContinuousSonification(ps.Sonification):\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        pass\n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.s.msg(\"/d_load\", [S2_PATH], bundle=True)\n",
    "    \n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.au4_node_id = self.s.node_ids.allocate(1)[0]\n",
    "        self.s.msg(\"/s_new\", [\"s2\", self.au4_node_id, 0, 0, \"amp\", 0], bundle=True)\n",
    "    \n",
    "    @bundle\n",
    "    def stop(self):\n",
    "        self.s.node_ids.free([self.au4_node_id])    # actually this does nothing\n",
    "        self.s.msg(\"/g_freeAll\", [0], bundle=True)\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        # only \"max\" should be enough (to clip the top part to 0.3)\n",
    "        amp = scn.linlin(row[\"AU04_r\"], 0, 1, 0, 0.3, \"minmax\")\n",
    "        # map the intensity of the AU in one octave range\n",
    "        freq = scn.midicps(scn.linlin(row[\"AU04_r\"], 0, 5, 69, 81))\n",
    "        self.s.msg(\"/n_set\", [self.au4_node_id, \"amp\", amp, \"freq\", freq], bundle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8fb04",
   "metadata": {},
   "source": [
    "#### High-level style (implicit ID allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb32da4",
   "metadata": {},
   "source": [
    "##### Explicit bundling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc3nb.osc.osc_communication import Bundler\n",
    "from sc3nb import SynthDef\n",
    "\n",
    "\n",
    "class AU04ContinuousSonification(ps.Sonification):\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        pass\n",
    "\n",
    "    def init_server(self):\n",
    "        with Bundler(send_on_exit=False) as bundler:\n",
    "            SynthDef.load(S2_PATH)\n",
    "        return bundler\n",
    "\n",
    "    def start(self):\n",
    "        with Bundler(send_on_exit=False) as bundler:\n",
    "            self.synth = scn.Synth(\"s2\", {\"amp\": 0})\n",
    "        return bundler\n",
    "    \n",
    "    def stop(self):\n",
    "        with Bundler(send_on_exit=False) as bundler:\n",
    "            self.s.free_all()\n",
    "        return bundler\n",
    "    \n",
    "    def _process(self, row):\n",
    "        with Bundler(send_on_exit=False) as bundler:\n",
    "            self.synth.set(\n",
    "                # only \"max\" should be enough (to clip the top part to 0.3)\n",
    "                \"amp\", scn.linlin(row[\"AU04_r\"], 0, 1, 0, 0.3, \"minmax\"),\n",
    "                # map the intensity of the AU in one octave range\n",
    "                \"freq\", scn.midicps(scn.linlin(row[\"AU04_r\"], 0, 5, 69, 81))\n",
    "            )\n",
    "        return bundler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da589a",
   "metadata": {},
   "source": [
    "##### Implicit bundling\n",
    "\n",
    "This is the most high level programming style that sc3nb allows, hence it is the **recommended**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b29fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc3nb import SynthDef\n",
    "\n",
    "# a (implicit ID allocation)\n",
    "class AU04ContinuousSonification(ps.Sonification):\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        pass\n",
    "\n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        SynthDef.load(S2_PATH)\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.synth = scn.Synth(\"s2\", {\"amp\": 0})\n",
    "    \n",
    "    @bundle\n",
    "    def stop(self):\n",
    "        self.s.free_all()\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        self.synth.set(\n",
    "            # only \"max\" should be enough (to clip the top part to 0.3)\n",
    "            \"amp\", self.amp * scn.linlin(row[self.label], 0, 1, 0, 0.3, \"minmax\"),\n",
    "            # map the intensity of the AU in one octave range\n",
    "            \"freq\", scn.midicps(scn.linlin(row[self.label], 0, 5, 69, 81))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df7952",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "It is possible to specify live preprocessing algorithms to be used in real-time by subclassing the **Preprocessor** abstract class.\n",
    "\n",
    "The following example will define a moving average with a window size of 10. This preprocessor will change the values of every data row obtained from the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71607eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features we want to compute the average of (AU intensities, head pose and gaze)\n",
    "avg_features = df.filter(regex='(AU.{2}_r)|(pose_R.)|(gaze_angle_.)').columns.to_list()\n",
    "# avg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497995ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAverage(ps.Preprocessor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.avg_features = avg_features\n",
    "        \n",
    "        self.window_size = 10\n",
    "        self.window = pd.DataFrame()\n",
    "    \n",
    "    def preprocess(self, row: pd.Series):            \n",
    "        self.window = self.window.append(row[self.avg_features])\n",
    "    \n",
    "        if self.window.shape[0] > self.window_size:\n",
    "            self.window = self.window.drop(self.window.index[0])\n",
    "\n",
    "        mean_series = self.window.mean()\n",
    "\n",
    "        # write values into row\n",
    "        for label in self.avg_features:\n",
    "            row[label] = mean_series[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918f9e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_openface_stream = streams.CsvFifo('openface', args=(FIFO_PATH,), preprocessor=MovingAverage)\\\n",
    "        .add_open_hook(feature_extraction_online)\\\n",
    "        .add_close_hook(kill_feature_extraction_online)\\\n",
    "        .test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c962c08",
   "metadata": {},
   "source": [
    "## Sonifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f1a1c",
   "metadata": {},
   "source": [
    "### Sonification: AU04 Test\n",
    "\n",
    "This sonification is similar to the previous sonifications, but with the addition of a parameter for amplitude regulation.\n",
    "\n",
    "This is a very simple sonification of AU04 (Brow Lowerer). The intensity of AU04 is used here to modulate both the amplitude and the frequency of a continuous synth. As continuous synth, the default synth of sc3nb s2 is used (we will have to instruct the server to load it).\n",
    "\n",
    "* The intensity range \\[0,1\\] is mapped into the amplitude range \\[0,0.3\\], where 0.3 will be the maximum amplitude of the sound. The sonification has a parameter amp that can be used to scale this range.\n",
    "* The intensity range \\[0,5\\] is mapped into the midi range \\[69,81\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AU04ContinuousSonification(ps.Sonification):\n",
    "    \n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1, 0.01)\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        self.amp = 1\n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.s.load_synthdefs()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        # lag time is decided based on the frame rate\n",
    "        self.synth = scn.Synth(\"s2\", {\"amp\": 0, \"lg\": 0.03})\n",
    "\n",
    "    @bundle\n",
    "    def _process(self, row):  \n",
    "        self.synth.set(\n",
    "            # only \"max\" should be enough (to clip the top part to 0.3)\n",
    "            \"amp\", self.amp * scn.linlin(row[\"AU04_r\"], 0, 1, 0, 0.3, \"minmax\"),\n",
    "            # map the intensity of the AU in one octave range\n",
    "            \"freq\", scn.midicps(scn.linlin(row[\"AU04_r\"], 0, 5, 69, 81))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323abc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = AU04ContinuousSonification()\n",
    "son"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f7269a",
   "metadata": {},
   "source": [
    "#### Offline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77326458",
   "metadata": {},
   "source": [
    "We can play the sonification on offline data along with the original video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47466fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = ps.VideoPlayer('path/to/video', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de0c54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_display = ps.RTFeatureDisplay(['AU04_r', 'AU12_r'], queue_size=50)\n",
    "# dp = ps.DataPlayer(son, feature_display=feature_display, video_player=vp).load(df)\n",
    "dp = ps.DataPlayer(son, feature_display=feature_display).load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed3076",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# feature_display.show(fps=30)\n",
    "# display(son)\n",
    "display(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a7d5f",
   "metadata": {},
   "source": [
    "#### Online\n",
    "\n",
    "Play sonification on real-time data using **openface_stream** (defined before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc8c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = AU04ContinuousSonification()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_display = ps.RTFeatureDisplay(['AU04_r'], 80)\n",
    "rtdp = ps.RTDataPlayer(openface_stream, son, feature_display=feature_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb73c75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_display.show(fps=10)\n",
    "display(son)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380573d",
   "metadata": {},
   "source": [
    "### Sonification: Pentatonic eyes + brows\n",
    "\n",
    "Here we are considering only the upper part of the face. More precisely, the following AUs.\n",
    "* 1: Inner Brow Raiser\n",
    "* 2: Outer Brow Raiser (unilateral)\n",
    "* 4: Brow Lowerer\n",
    "* 5: Upper Lid Raiser\n",
    "* 6: Cheek Raiser\n",
    "* 7: Lid Tightener\n",
    "\n",
    "With the following sonification, we'll be able to control some synths playing a minor pentatonic scale with the upper part of our faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92995e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PentatonicContinuous(ps.Sonification):\n",
    "\n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1, 0.01)\n",
    "    fundamental = ps.MidiSliderParameter()\n",
    "    \n",
    "    AUs_offsets = {\n",
    "        'AU01_r': 12,\n",
    "        'AU02_r': 10,\n",
    "        'AU04_r': 7,\n",
    "        'AU05_r': 5,\n",
    "        'AU06_r': 0,\n",
    "        'AU07_r': 3\n",
    "    }\n",
    "\n",
    "    def init_parameters(self):\n",
    "        self.amp = 0.3\n",
    "        self.fundamental = 69\n",
    "\n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.s.load_synthdefs()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.synths = {}\n",
    "        \n",
    "        for label, offset in self.AUs_offsets.items():\n",
    "            self.synths[label] = scn.Synth(\"s2\", {\"amp\": 0, \"freq\": scn.midicps(self.fundamental + offset), \"lg\": 0.015})\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        for label, offset in self.AUs_offsets.items():\n",
    "            intensity = row[label]\n",
    "            amp = self.map_intensity(intensity)\n",
    "            self.synths[label].set(\n",
    "                \"amp\", self.amp * amp,\n",
    "                \"freq\", scn.midicps(self.fundamental + offset)\n",
    "            )\n",
    "            \n",
    "        \n",
    "    @staticmethod\n",
    "    def map_intensity(intensity):\n",
    "        if intensity < 1.0:\n",
    "            amp = 0\n",
    "        else:\n",
    "            db = scn.linlin(intensity, 1, 5, -20, -5, \"minmax\")\n",
    "            amp = scn.dbamp(db)\n",
    "\n",
    "        return amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f706fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = PentatonicContinuous()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66df491",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_features = [\"AU01_r\", \"AU02_r\", \"AU04_r\", \"AU05_r\", \"AU06_r\", \"AU07_r\"]\n",
    "feature_display = ps.RTFeatureDisplay(interesting_features, 80)\n",
    "\n",
    "rtdp = ps.RTDataPlayer(openface_stream, son, feature_display=feature_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_display.show(fps=10)\n",
    "display(son)\n",
    "display(rtdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffe905",
   "metadata": {},
   "source": [
    "#### With MdaPiano\n",
    "\n",
    "This sonification implement the same logic of the previous one, but using event-based sonification. Here, every time that the intensity of a certain AU crosses the integer thresholds (1, 2, 3, 4), a piano note with amplitude depending on the intensity is triggered. This happens both when the intensity is increasing and when it is decreasing, which can be confusing.\n",
    "\n",
    "As OpenFace's intensity prediction can oscillate quite a lot, it is likely that some features will oscillate around grid tresholds, causing the sound to be retriggered multiple times. To alleviate this problem we will use the stream object that executes the moving average as preprocessing.\n",
    "\n",
    "Note: MdaPiano is part of SuperCollider extensions. Make sure it's installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869eeac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PentatonicMda(ps.Sonification):\n",
    "    \n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1, 0.01)\n",
    "    fundamental = ps.MidiSliderParameter()\n",
    "    \n",
    "    piano_def = scn.SynthDef(\n",
    "        \"mdapiano\",\n",
    "        r\"\"\"{ |freq=440, vel=100, amp=1|\n",
    "            var piano = MdaPiano.ar(\n",
    "                freq,\n",
    "                1,\n",
    "                vel,\n",
    "                decay: 0,\n",
    "                release: 0,\n",
    "                hard: 0,\n",
    "                stereo: 0,\n",
    "                mul: amp\n",
    "            );\n",
    "            // this can lead to artifacts, it would be better to fade out with an envelope\n",
    "            DetectSilence.ar(piano, 0.05, doneAction:2);\n",
    "            Out.ar(0, piano);\n",
    "        }\"\"\"\n",
    "    )\n",
    "    \n",
    "    AUs_offsets = {\n",
    "        'AU01_r': 12,\n",
    "        'AU02_r': 10,\n",
    "        'AU04_r': 7,\n",
    "        'AU05_r': 5,\n",
    "        'AU06_r': 0,\n",
    "        'AU07_r': 3\n",
    "    }\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        self.amp = 0.3\n",
    "        self.fundamental = 69\n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.piano_def.add()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.range_level = {}\n",
    "        \n",
    "        for label in self.AUs_offsets.keys():\n",
    "            self.range_level[label] = 0\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        for label, offset in self.AUs_offsets.items():\n",
    "            intensity = row[label]\n",
    "            cur_range_level = int(intensity)\n",
    "            \n",
    "            freq = scn.midicps(self.fundamental + offset)\n",
    "            \n",
    "            if cur_range_level != self.range_level[label] and cur_range_level >= 1:\n",
    "                vel = scn.linlin(cur_range_level, 1, 5, 40, 127)\n",
    "                scn.Synth('mdapiano', {\"freq\": freq, \"vel\": vel * self.amp})\n",
    "            self.range_level[label] = cur_range_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f518370",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = PentatonicMda()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd0c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rtdp = ps.RTDataPlayer(avg_openface_stream, son, feature_display=feature_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f1adb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_display.show()\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400898f7",
   "metadata": {},
   "source": [
    "### Sonification: Drop blink\n",
    "\n",
    "The following sonifications sonifies blinking with a synthesized drop sound. OpenFace considers the blinking active when the intensity falue is greater or equal to 1.\n",
    "\n",
    "To reduce the effect of oscillating features, we will establish different threshold for activation and deactivation of blinking. By default, these will be 1.6 and 1, meaning that:\n",
    "* blinking will switch on only when 1.6 of intensity is exceeded while the blinking was off\n",
    "* blinking will switch off only when 1 of intensity is exceeded (going down) while the blinking was on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1632604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropBlink(ps.Sonification):\n",
    "    \n",
    "    # hysteresis boundaries\n",
    "    bounds = ps.FloatRangeSliderParameter(0, 5, 0.1)\n",
    "    \n",
    "    # drop definition\n",
    "    drop_def = scn.SynthDef(\n",
    "        \"drop\",\n",
    "        r\"\"\"{ | freq=600, dp=1200, amp=0.5, dur=0.1, pan=0 |\n",
    "            var sig, env, fch;\n",
    "            fch = XLine.kr(freq, freq+dp, dur);\n",
    "            sig = SinOsc.ar(fch);\n",
    "            env = EnvGen.kr(Env.perc(0.001, dur, curve: -4), 1.0, doneAction: 2);\n",
    "            Out.ar(0, Pan2.ar(sig, pan, env*amp))\n",
    "        }\"\"\"\n",
    "    )\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        self.bounds = [1, 1.6]        \n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.drop_def.add()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.blinking = False\n",
    "    \n",
    "    @bundle\n",
    "    def stop(self):\n",
    "        # drops die out alone\n",
    "        pass\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        intensity = row[\"AU45_r\"]\n",
    "        \n",
    "        if self.blinking:\n",
    "            if intensity < self.bounds[0]:\n",
    "                self.blinking = False\n",
    "        elif intensity > self.bounds[1]:\n",
    "            self.blinking = True\n",
    "            scn.Synth(\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = DropBlink()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_display = ps.RTFeatureDisplay([\"AU45_r\"], 80)\n",
    "\n",
    "rtdp = ps.RTDataPlayer(openface_stream, son, feature_display=feature_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6635e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_display.show(10)\n",
    "display(son)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf80d63",
   "metadata": {},
   "source": [
    "The following associates a blink to the closing of the eyes and a blink to the opening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDropBlink(DropBlink):\n",
    "\n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        intensity = row[\"AU45_r\"]\n",
    "        \n",
    "        if self.blinking:\n",
    "            if intensity < self.bounds[0]:\n",
    "                self.blinking = False\n",
    "                scn.Synth(\"drop\", {\"freq\": 900})\n",
    "        elif intensity > self.bounds[1]:\n",
    "            self.blinking = True\n",
    "            scn.Synth(\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aef9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = DoubleDropBlink()\n",
    "son"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2e991",
   "metadata": {},
   "source": [
    "### Multi-percussion (event-based)\n",
    "\n",
    "Hereafter it is shown how to implement event-based sonifications using samples.\n",
    "\n",
    "Every sample is triggered when crossing a grid threshold; this is the same thing that we did in the PentatonicMda sonification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2f509",
   "metadata": {},
   "source": [
    "Concepts:\n",
    "* Sounds that usually plays together should be distinguishable\n",
    "* Sounds that play more often should be less intrusive\n",
    "* One area should have sounds that are somehow related\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 1: Inner Brow Raiser - <audio controls src=\"samples/au01.wav\"/>\n",
    "* 2: Outer Brow Raiser (unilateral) - <audio controls src=\"samples/au02.wav\"/>\n",
    "* 4: Brow Lowerer - <audio controls src=\"samples/au04.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 5: Upper Lid Raiser - <audio controls src=\"samples/au05.wav\"/>\n",
    "* 6: Cheek Raiser - <audio controls src=\"samples/au06.wav\"/>\n",
    "* 7: Lid Tightener - <audio controls src=\"samples/au07.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 9: Nose Wrinkler (usually goes along with 4 and 10) - <audio controls src=\"samples/au09.wav\"/>\n",
    "* 10: Upper Lip Raiser - <audio controls src=\"samples/au10.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 12: Lip Corner Puller - <audio controls src=\"samples/au12.wav\"/>\n",
    "* 14: Dimpler - <audio controls src=\"samples/au14.wav\"/>\n",
    "* 15: Lip Corner Depressor - <audio controls src=\"samples/au15.wav\"/>\n",
    "* 20: Lip Stretcher - <audio controls src=\"samples/au20.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 23: Lip Tightener - <audio controls src=\"samples/au23.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 17: Chin Raiser - <audio controls src=\"samples/au17.wav\"/>\n",
    "* 25: Lips Part (relax Mentalis, antagonist of AU17) - <audio controls src=\"samples/au25.wav\"/>\n",
    "* 26: Jaw Drop (usually goes along with 25) - <audio controls src=\"samples/au26.wav\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "* 28: Lip Suck (usually along with 26) - <audio controls src=\"samples/au28.wav\"/>\n",
    "\n",
    "    * OpenFace only provides presence information on this, which is not really reliable in a dynamic context; for this reason, we will omit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270cf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AU01_SAMPLE_PATH = \"samples/au01.wav\"\n",
    "AU02_SAMPLE_PATH = \"samples/au02.wav\"\n",
    "AU04_SAMPLE_PATH = \"samples/au04.wav\"\n",
    "AU05_SAMPLE_PATH = \"samples/au05.wav\"\n",
    "AU06_SAMPLE_PATH = \"samples/au06.wav\"\n",
    "AU07_SAMPLE_PATH = \"samples/au07.wav\"\n",
    "AU09_SAMPLE_PATH = \"samples/au09.wav\"\n",
    "AU10_SAMPLE_PATH = \"samples/au10.wav\"\n",
    "AU12_SAMPLE_PATH = \"samples/au12.wav\"\n",
    "AU14_SAMPLE_PATH = \"samples/au14.wav\"\n",
    "AU15_SAMPLE_PATH = \"samples/au15.wav\"\n",
    "AU17_SAMPLE_PATH = \"samples/au17.wav\"\n",
    "AU20_SAMPLE_PATH = \"samples/au20.wav\"\n",
    "AU23_SAMPLE_PATH = \"samples/au23.wav\"\n",
    "AU25_SAMPLE_PATH = \"samples/au25.wav\"\n",
    "AU26_SAMPLE_PATH = \"samples/au26.wav\"\n",
    "AU28_SAMPLE_PATH = \"samples/au28.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80afaf5",
   "metadata": {},
   "source": [
    "#### Sonification: Percussive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49967b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Percussive(ps.Sonification):\n",
    "    \n",
    "    amp = ps.FloatSliderParameter(0, 1, 0.01)\n",
    "\n",
    "    playbuf_def = scn.SynthDef(\n",
    "        \"playbuf\",\n",
    "        r\"\"\"{| out=0, bufnum=0, rate=1, amp=1, pan=0 |\n",
    "            var sig;\n",
    "            sig = PlayBuf.ar(1, bufnum, rate*BufRateScale.kr(bufnum), doneAction:2);\n",
    "            Out.ar(0, Pan2.ar(sig, pan, amp));\n",
    "        }\"\"\"\n",
    "    )\n",
    "    \n",
    "    AUs_samples = {\n",
    "        'AU01_r': AU01_SAMPLE_PATH,\n",
    "        'AU02_r': AU02_SAMPLE_PATH,\n",
    "        'AU04_r': AU04_SAMPLE_PATH,\n",
    "        'AU05_r': AU05_SAMPLE_PATH,\n",
    "        'AU06_r': AU06_SAMPLE_PATH,\n",
    "        'AU07_r': AU07_SAMPLE_PATH,\n",
    "        'AU09_r': AU09_SAMPLE_PATH,\n",
    "        'AU10_r': AU10_SAMPLE_PATH,\n",
    "        'AU12_r': AU12_SAMPLE_PATH,\n",
    "        'AU14_r': AU14_SAMPLE_PATH,\n",
    "        'AU15_r': AU15_SAMPLE_PATH,\n",
    "        'AU17_r': AU17_SAMPLE_PATH,\n",
    "        'AU20_r': AU20_SAMPLE_PATH,\n",
    "        'AU23_r': AU23_SAMPLE_PATH,\n",
    "        'AU25_r': AU25_SAMPLE_PATH,\n",
    "        'AU26_r': AU26_SAMPLE_PATH\n",
    "    }\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        self.amp = 0.3\n",
    "\n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.playbuf_def.add()\n",
    "        \n",
    "        self.buffers = {}\n",
    "        \n",
    "        for label, sample_path in self.AUs_samples.items():\n",
    "            self.buffers[label] = scn.Buffer().read(sample_path)\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.range_level = {}\n",
    "        \n",
    "        for label in self.AUs_samples.keys():\n",
    "            self.range_level[label] = 0\n",
    "    \n",
    "    @bundle\n",
    "    def stop(self):\n",
    "        # synths die out alone\n",
    "        pass\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        \n",
    "        for label in self.AUs_samples.keys():\n",
    "            # cast intensity to integer\n",
    "            cur_range_level = int(row[label])\n",
    "\n",
    "            if cur_range_level != self.range_level[label] and cur_range_level >= 1:\n",
    "                db = scn.linlin(cur_range_level, 1, 5, -40, 0, \"minmax\")\n",
    "                scn.Synth(\"playbuf\", {\"bufnum\": self.buffers[label].bufnum, \"amp\": scn.dbamp(db) * self.amp})\n",
    "\n",
    "            # update old_range_level\n",
    "            self.range_level[label] = cur_range_level\n",
    "            \n",
    "    def free(self):\n",
    "        # deallocate buffers\n",
    "        for buf in self.buffers.values():\n",
    "            buf.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = Percussive()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d1cff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rtdp = ps.RTDataPlayer(openface_stream, son)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "son.free()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbd51a",
   "metadata": {},
   "source": [
    "#### Sonification: Multi-percussion sound with intensity direction\n",
    "\n",
    "This is a refined version of the previous one. Samples will be played with different properties according to whether the intensity of the AUs is increasing (the sample will bend up) or decreasing (the sample will bend up). It is also possible to distinguish them using panning.\n",
    "\n",
    "As we can see **init_parameters** here will take some arguments. These arguments will be passed to the function through the constructor, so that they can be specified when instantiating the sonification.\n",
    "\n",
    "To limit the noise due to feature oscillation, we use the same approach that we used in the blinking sonification, but specifying limits that are relative to each grid level. These can be manipulated through the **bounds** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectionalPercussive(ps.Sonification):\n",
    "    \n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1, 0.01)\n",
    "    pan = ps.CheckboxParameter()\n",
    "    \n",
    "    # hysteresis bounds relative to each intensity level\n",
    "    bounds = ps.FloatRangeSliderParameter(-1, +1)\n",
    "\n",
    "    playbuf_def = scn.SynthDef(\n",
    "        \"playbuf_bend\",\n",
    "        r\"\"\"{| out=0, bufnum=0, rateInitial=1, amp=1, pan=0, breakTime, rateFinal |\n",
    "            var sig, rate;\n",
    "\n",
    "            var rateAvg = (rateInitial + rateFinal) / 2;\n",
    "            var sampleRateAvg = rateAvg * BufSampleRate.kr(bufnum);\n",
    "            var breakFrame = breakTime * BufSampleRate.kr(bufnum);\n",
    "            // mono signal: frames = samples\n",
    "            var remainingFrames = BufSamples.kr(bufnum) - breakFrame;\n",
    "            // calculate remaining time with dynamic rate\n",
    "            var remainingTime = remainingFrames / sampleRateAvg;\n",
    "\n",
    "            rate = EnvGen.kr(\n",
    "                Env(\n",
    "                    [rateInitial, rateInitial, rateFinal],\n",
    "                    [breakTime, remainingTime]\n",
    "                )\n",
    "            );\n",
    "            sig = PlayBuf.ar(1, bufnum, rate*BufRateScale.kr(bufnum), doneAction:2);\n",
    "            Out.ar(0, Pan2.ar(sig, pan, amp));\n",
    "        }\"\"\"\n",
    "    )\n",
    "    \n",
    "    AuRecord = namedtuple('AuRecord', ['path', 'break_time', 'rate_up', 'rate_down'])\n",
    "    \n",
    "    AUs = {\n",
    "        'AU01_r': AuRecord(AU01_SAMPLE_PATH, 0.2, 1.1, 0.9),\n",
    "        'AU02_r': AuRecord(AU02_SAMPLE_PATH, 0.2, 1.1, 0.9),\n",
    "        'AU04_r': AuRecord(AU04_SAMPLE_PATH, 0.2, 1.1, 0.9),\n",
    "        'AU05_r': AuRecord(AU05_SAMPLE_PATH, 0.05, 1.1, 0.9),\n",
    "        'AU06_r': AuRecord(AU06_SAMPLE_PATH, 0.03, 1.1, 0.9),\n",
    "        'AU07_r': AuRecord(AU07_SAMPLE_PATH, 0.1, 1.1, 0.9),\n",
    "        'AU09_r': AuRecord(AU09_SAMPLE_PATH, 0.2, 1.5, 0.7),\n",
    "        'AU10_r': AuRecord(AU10_SAMPLE_PATH, 0.15, 1.2, 0.9),\n",
    "        'AU12_r': AuRecord(AU12_SAMPLE_PATH, 0.1, 1.1, 0.9),\n",
    "        'AU14_r': AuRecord(AU14_SAMPLE_PATH, 0.1, 1.1, 0.9),\n",
    "        'AU15_r': AuRecord(AU15_SAMPLE_PATH, 0.05, 1.1, 0.9),\n",
    "        'AU17_r': AuRecord(AU17_SAMPLE_PATH, 0.05, 1.5, 0.8),\n",
    "        'AU20_r': AuRecord(AU20_SAMPLE_PATH, 0.1, 1.1, 0.9),\n",
    "        'AU23_r': AuRecord(AU23_SAMPLE_PATH, 0.15, 1.25, 0.85),\n",
    "        'AU25_r': AuRecord(AU25_SAMPLE_PATH, 0.005, 2, 0.5),\n",
    "        'AU26_r': AuRecord(AU26_SAMPLE_PATH, 0.05, 2, 0.5)\n",
    "    }\n",
    "    \n",
    "    def init_parameters(self, pan=False):\n",
    "        self.amp = 0.3\n",
    "        self.pan = pan\n",
    "        self.bounds = [-0.3, +0.3]\n",
    "\n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.playbuf_def.add()\n",
    "        \n",
    "        self.buffers = {}\n",
    "        \n",
    "        # allocate buffers\n",
    "        for label, record in self.AUs.items():\n",
    "            self.buffers[label] = scn.Buffer().read(record.path)\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.range_level = {}\n",
    "        \n",
    "        for label in self.AUs.keys():\n",
    "            self.range_level[label] = 0\n",
    "    \n",
    "    @bundle\n",
    "    def stop(self):\n",
    "        # synths die out alone\n",
    "        pass\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        \n",
    "        for label, record in self.AUs.items():\n",
    "            intensity = row[label]\n",
    "            cur_range_level = self.map_intensity(intensity, self.range_level[label])\n",
    "            \n",
    "            if cur_range_level != self.range_level[label] and cur_range_level >= 1:\n",
    "                db = scn.linlin(cur_range_level, 1, 5, -40, 0, \"minmax\")\n",
    "                amp = scn.dbamp(db)\n",
    "            \n",
    "                if cur_range_level > self.range_level[label]:\n",
    "                    scn.Synth(\n",
    "                        \"playbuf_bend\",\n",
    "                        {\n",
    "                            \"bufnum\": self.buffers[label].bufnum,\n",
    "                            \"amp\": self.amp * amp,\n",
    "                            \"pan\": 1 if self.pan else 0,\n",
    "                            \"breakTime\": record.break_time,\n",
    "                            \"rateFinal\": record.rate_up\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    scn.Synth(\n",
    "                        \"playbuf_bend\",\n",
    "                        {\n",
    "                            \"bufnum\": self.buffers[label].bufnum,\n",
    "                            \"amp\": self.amp * amp,\n",
    "                            \"pan\": -1 if self.pan else 0,\n",
    "                            \"breakTime\": record.break_time,\n",
    "                            \"rateFinal\": record.rate_down\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "            # update old_range_level\n",
    "            self.range_level[label] = cur_range_level\n",
    "\n",
    "    def map_intensity(self, intensity, old_level):\n",
    "        cur_level = int(intensity)\n",
    "        \n",
    "        if cur_level == old_level:\n",
    "            return cur_level        \n",
    "        elif cur_level > old_level:\n",
    "            return cur_level if intensity > cur_level + self.bounds[1] else old_level\n",
    "        else:\n",
    "            return cur_level if intensity < old_level + self.bounds[0] else old_level\n",
    "            \n",
    "    def free(self):\n",
    "        # deallocate buffers\n",
    "        for buf in self.buffers.values():\n",
    "            buf.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = DirectionalPercussive(pan=True)\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca03973",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rtdp = ps.RTDataPlayer(openface_stream, son)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25897bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "son.free()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fb4fe",
   "metadata": {},
   "source": [
    "### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_display = ps.RTFeatureDisplay(['pose_Rx', 'pose_Ry', 'pose_Rz'], queue_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babbc6fa",
   "metadata": {},
   "source": [
    "#### Sonification: Head rotations with constant amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "class Head(ps.Sonification):\n",
    "    \n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1, 0.01)\n",
    "            \n",
    "    def init_parameters(self):\n",
    "        self.amp = 0.1\n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.s.load_synthdefs()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.synth = scn.Synth(\"s2\", {\"amp\": 0, \"lg\": 0.015})\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        \n",
    "        pitch = scn.linlin(row['pose_Rx'], -pi/4, +pi/4, 69+12, 69-12)\n",
    "        pan =   scn.linlin(row['pose_Ry'], -pi/4, +pi/4, +1, -1)\n",
    "        num =   scn.linlin(row['pose_Rz'], -pi/4, +pi/4, 1, 7)\n",
    "        \n",
    "        self.synth.set(\n",
    "            \"amp\", self.amp,\n",
    "            \"freq\", scn.midicps(pitch),\n",
    "            \"pan\", pan,\n",
    "            \"num\", num\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = Head()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdp = ps.RTDataPlayer(openface_stream, son, feature_display)\n",
    "feature_display.show(5)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad780c28",
   "metadata": {},
   "source": [
    "#### Sonification: Head rotations with silent neutral position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db24c7",
   "metadata": {},
   "source": [
    "The amplitude is mapped linearly based on the biggest rotation.\n",
    "* a linear mapping makes quiet sounds more distinguishable\n",
    "* every axe has a silence threshold, so that a head in a pretty neutral position does not generate any sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a143c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilentHead(ps.Sonification):\n",
    "    \n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1, 0.01)\n",
    "    \n",
    "    # max expected values for each rotation\n",
    "    rx_bound = pi/4\n",
    "    ry_bound = pi/4\n",
    "    rz_bound = pi/4\n",
    "    \n",
    "    # in radians\n",
    "    rx_silence_thrashold = ps.FloatSliderParameter(0, rx_bound)\n",
    "    ry_silence_thrashold = ps.FloatSliderParameter(0, ry_bound)\n",
    "    rz_silence_thrashold = ps.FloatSliderParameter(0, rz_bound)\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        self.amp = 0.3\n",
    "        \n",
    "        self.rx_silence_thrashold = self.rx_bound / 10\n",
    "        self.ry_silence_thrashold = self.ry_bound / 10\n",
    "        self.rz_silence_thrashold = self.rz_bound / 10\n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.s.load_synthdefs()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.synth = scn.Synth(\"s2\", {\"amp\": 0, \"lg\": 0.015})\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        \n",
    "        rx, ry, rz = row[['pose_Rx', 'pose_Ry', 'pose_Rz']]\n",
    "\n",
    "        amp = max(\n",
    "            # clips values under silence thrashold\n",
    "            scn.linlin(abs(rx), self.rx_silence_thrashold, +self.rx_bound, 0, 1, \"minmax\"),\n",
    "            scn.linlin(abs(ry), self.ry_silence_thrashold, +self.ry_bound, 0, 1, \"minmax\"),\n",
    "            scn.linlin(abs(rz), self.rx_silence_thrashold, +self.rz_bound, 0, 1, \"minmax\")\n",
    "        )\n",
    "        \n",
    "        pitch = scn.linlin(rx, -self.rx_bound, +self.rx_bound, 69+12, 69-12)\n",
    "        pan =   scn.linlin(ry, -self.ry_bound, +self.ry_bound, +1, -1)\n",
    "        num =   scn.linlin(rz, -self.rz_bound, +self.rz_bound, 1, 7)\n",
    "        \n",
    "        self.synth.set(\n",
    "            \"amp\", self.amp * amp,\n",
    "            \"freq\", scn.midicps(pitch),\n",
    "            \"pan\", pan,\n",
    "            \"num\", num\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1819c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = SilentHead()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdp = ps.RTDataPlayer(openface_stream, son, feature_display)\n",
    "feature_display.show(5)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec9006",
   "metadata": {},
   "source": [
    "#### Sonification: Noisy head rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyHead(ps.Sonification):\n",
    "    \n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1, 0.01)\n",
    "    base_tone = ps.MidiSliderParameter()\n",
    "\n",
    "    mirror = ps.CheckboxParameter()\n",
    "    \n",
    "    # max expected values for each rotation\n",
    "    rx_bound = ps.FloatSliderParameter(pi/2 / 10, pi/2)\n",
    "    ry_bound = ps.FloatSliderParameter(pi/2 / 10, pi/2)\n",
    "    rz_bound = ps.FloatSliderParameter(pi/2 / 10, pi/2)\n",
    "    \n",
    "    synth_def = scn.SynthDef(\n",
    "        \"bpf_noise\",\n",
    "        r\"\"\"{ | amp=1, pan=0, lg=0.5, freq=440, rq=0.2 |\n",
    "            var sig;\n",
    "            sig = PinkNoise.ar(amp);\n",
    "            sig = BPF.ar(\n",
    "                sig,\n",
    "                freq.lag(lg),\n",
    "                rq.lag(lg),\n",
    "                // when a bandpass filter narrows, the amplitude decreases: this will balance it\n",
    "                1/rq.sqrt.lag(lg)\n",
    "            );\n",
    "            Out.ar(0, Pan2.ar(sig, pan));\n",
    "        }\"\"\"\n",
    "    )\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        self.amp = 0.3\n",
    "        self.base_tone = 69\n",
    "        self.log_mapping = False\n",
    "        self.mirror = True\n",
    "        \n",
    "        self.rx_bound = pi/4\n",
    "        self.ry_bound = pi/4\n",
    "        self.rz_bound = pi/4\n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.synth_def.add()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.synth = scn.Synth(\"bpf_noise\", {\"amp\": 0, \"lg\": 0.015})\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        \n",
    "        rx, ry, rz = row[['pose_Rx', 'pose_Ry', 'pose_Rz']]\n",
    "        \n",
    "        pitch = scn.linlin(rx, -self.rx_bound, +self.rx_bound, self.base_tone+12, self.base_tone-12, \"minmax\")\n",
    "        pan = scn.linlin(ry, -self.ry_bound, +self.ry_bound, +1, -1, \"minmax\")\n",
    "        # linexp mapping to quality\n",
    "        q = 2 ** (scn.linlin(rz, -self.rz_bound, +self.rz_bound, log2(1), log2(100), 'minmax'))\n",
    "            \n",
    "        if self.mirror:\n",
    "            pan = -pan\n",
    "            # q   = 100 - q + 1\n",
    "        \n",
    "        self.synth.set(\n",
    "            \"amp\", self.amp,\n",
    "            \"freq\", scn.midicps(pitch),\n",
    "            \"pan\", pan,\n",
    "            \"rq\", 1/q\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = NoisyHead()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963f42c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rtdp = ps.RTDataPlayer(openface_stream, son, feature_display)\n",
    "# feature_display.show(5)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd52e1",
   "metadata": {},
   "source": [
    "### Gaze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa7f98",
   "metadata": {},
   "source": [
    "**gaze_N_x, gaze_N_y, gaze_N_z**: Eye gaze **direction vector** in world coordinates for eye left and right eye (normalized)\n",
    "- N = 0: leftmost eye in the image\n",
    "- N = 1: rightmost eye in the image\n",
    "\n",
    "**gaze_angle_x, gaze_angle_y**: Eye gaze direction in radians in world coordinates averaged for both eyes and converted into more **easy to use format** than gaze vectors.\n",
    "- If a person is looking left-right this will results in the change of gaze_angle_x (from positive to negative)\n",
    "- if a person is looking up-down this will result in change of gaze_angle_y (from negative to positive)\n",
    "- if a person is looking straight ahead both of the angles will be close to 0 (within measurement error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_display = ps.RTFeatureDisplay(['gaze_angle_x', 'gaze_angle_y'], queue_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1057d64f",
   "metadata": {},
   "source": [
    "#### Sonification: Gaze with constant amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a000a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaze(ps.Sonification):\n",
    "    \n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1, 0.01)\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        self.amp = 0.1\n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.s.load_synthdefs()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.synth = scn.Synth(\"s2\", {\"amp\": 0, \"lg\": 0.015})\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        \n",
    "        pitch = scn.linlin(row['gaze_angle_y'], -pi/2, +pi/2, 69+12, 69-12)\n",
    "        pan =   scn.linlin(row['gaze_angle_x'], -pi/2, +pi/2, +1, -1)\n",
    "        \n",
    "        self.synth.set(\n",
    "            \"amp\", self.amp,\n",
    "            \"freq\", scn.midicps(pitch),\n",
    "            \"pan\", pan\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be23b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = Gaze()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdp = ps.RTDataPlayer(openface_stream, son, feature_display)\n",
    "feature_display.show(5)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f6c3f",
   "metadata": {},
   "source": [
    "#### Sonification: Gaze with silent neutral position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilentGaze(ps.Sonification):\n",
    "    \n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1, 0.01)\n",
    "    \n",
    "    gx_silence_thrashold = ps.FloatSliderParameter(0, pi/2, 0.01)\n",
    "    gy_silence_thrashold = ps.FloatSliderParameter(0, pi/2, 0.01)\n",
    "    \n",
    "    fundamental = ps.MidiSliderParameter()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        self.amp = 0.1\n",
    "        self.gx_silence_thrashold = pi/2 / 10\n",
    "        self.gy_silence_thrashold = pi/2 / 10\n",
    "        \n",
    "        self.fundamental = 69\n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.s.load_synthdefs()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.synth = scn.Synth(\"s2\", {\"amp\": 0, \"lg\": 0.015})\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        \n",
    "        gx, gy = row[['gaze_angle_x', 'gaze_angle_y']]\n",
    "        \n",
    "        # linear mapping\n",
    "        amp = max(\n",
    "            # clips values under silence thrashold\n",
    "            scn.linlin(abs(gx), self.gx_silence_thrashold, pi/2, 0, 1, \"minmax\"),\n",
    "            scn.linlin(abs(gy), self.gy_silence_thrashold, pi/2, 0, 1, \"minmax\")\n",
    "        )\n",
    "        \n",
    "        pan =   scn.linlin(gx, -pi/2, +pi/2, -1, +1)\n",
    "        pitch = scn.linlin(gy, -pi/2, +pi/2, self.fundamental+12, self.fundamental-12)\n",
    "        \n",
    "        self.synth.set(\n",
    "            \"amp\", self.amp * amp,\n",
    "            \"freq\", scn.midicps(pitch),\n",
    "            \"pan\", pan\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bbd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = SilentGaze()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdp = ps.RTDataPlayer(openface_stream, son, feature_display)\n",
    "feature_display.show(5)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f647cd",
   "metadata": {},
   "source": [
    "### Smile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f2208",
   "metadata": {},
   "source": [
    "AUs regarding the smile recognised by OpenFace:\n",
    "* AU06 - Cheek raiser\n",
    "* AU12 - Lip Corner Puller\n",
    "* AU14 - Dimpler\n",
    "* AU15 - Lip Corner Depressor\n",
    "* AU17 - Chin Raiser\n",
    "\n",
    "Expected to go together:\n",
    "* AU06 - AU12\n",
    "* AU15 - AU17\n",
    "\n",
    "For now we will ignore AU14 for simplicity, and we will sonify it using the percussive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d3eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_display = ps.RTFeatureDisplay(['AU06_r', 'AU12_r', 'AU15_r', 'AU17_r'], queue_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d717a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DustSmile(ps.Sonification):\n",
    "    \n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1)\n",
    "    base_tone = ps.MidiSliderParameter()\n",
    "    \n",
    "    synth_def = scn.SynthDef(\n",
    "        \"discrete_rev\",\n",
    "        r\"\"\"{ | amp=1, freq=440, density=2, mix=0.5, room=0.5, damp=0.2, lg=0.1 |\n",
    "            var trig, sig, env;\n",
    "            sig = SinOsc.ar(freq);\n",
    "            // transform signal into short blips\n",
    "            trig = Dust.kr(density);\n",
    "            env = EnvGen.kr(Env.perc(0.001, 0.05), trig);\n",
    "            sig = sig * env;\n",
    "            sig = FreeVerb.ar(sig, mix.lag(lg), room.lag(lg), damp.lag(lg), amp.lag(lg));\n",
    "            Out.ar(0, sig!2);\n",
    "        }\"\"\"\n",
    "    )\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        self.amp = 0.3\n",
    "        self.base_tone = 69\n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.synth_def.add()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.synth = scn.Synth(\"discrete_rev\", {\"amp\": 0, \"density\": 0})\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        # intensities\n",
    "        au06, au12, au15, au17 = row[['AU06_r', 'AU12_r', 'AU15_r', 'AU17_r']]\n",
    "\n",
    "        if au12 > 1 and au15 > 1:\n",
    "            # this should not happen usually\n",
    "            return\n",
    "        \n",
    "        if au12 > 1:\n",
    "            pitch = scn.linlin(au12, 1, 5, self.base_tone, self.base_tone+12, 'minmax')\n",
    "            mix = scn.linlin(au06, 1, 3, 0.2, 1, 'minmax')\n",
    "            density = scn.linlin(max(au12, au06), 1, 5, 5, 30, 'minmax')\n",
    "        elif au15 > 1:\n",
    "            pitch = scn.linlin(au15, 1, 5, self.base_tone, self.base_tone-12, 'minmax')\n",
    "            mix = scn.linlin(au17, 1, 3, 0.2, 1, 'minmax')\n",
    "            density = scn.linlin(max(au15, au17), 1, 5, 5, 30, 'minmax')\n",
    "        else:\n",
    "            pitch = 69\n",
    "            mix = 0\n",
    "            density = 0\n",
    "        \n",
    "        self.synth.set(\n",
    "            \"amp\", self.amp,\n",
    "            \"freq\", scn.midicps(pitch),\n",
    "            \"mix\", mix,\n",
    "            \"density\", density,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56898c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = DustSmile()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdp = ps.RTDataPlayer(openface_stream, son, feature_display)\n",
    "feature_display.show(5)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8aca1",
   "metadata": {},
   "source": [
    "### Modular sonifications using GroupSonification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098996ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = ps.GroupSonification([DirectionalPercussive(), DropBlink(), NoisyHead(), SilentGaze()])\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef65d7f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rtdp = ps.RTDataPlayer(openface_stream, son)\n",
    "rtdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07338c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "son.free()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d335e78",
   "metadata": {},
   "source": [
    "## Multi-stream\n",
    "\n",
    "Panson provides support for processing multiple streams of data (with different frame rates) at the same time. Hereafter we show a very simple example: the amplitude of the sonification of facial features is modulated according to a stream that yields sinusoidal values at 20 fps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd3220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyHeadMulti(ps.Sonification):\n",
    "    \n",
    "    # parameters of the sonification\n",
    "    amp = ps.FloatSliderParameter(0, 1)\n",
    "    base_tone = ps.MidiSliderParameter()\n",
    "    log_mapping = ps.CheckboxParameter()\n",
    "    \n",
    "    # max expected values for each rotation\n",
    "    rx_bound = ps.FloatSliderParameter(pi/2 / 10, pi/2)\n",
    "    ry_bound = ps.FloatSliderParameter(pi/2 / 10, pi/2)\n",
    "    rz_bound = ps.FloatSliderParameter(pi/2 / 10, pi/2)\n",
    "    \n",
    "    synth_def = scn.SynthDef(\n",
    "        \"bpf_noise\",\n",
    "        r\"\"\"{ | amp=1, pan=0, lg=0.5, freq=440, rq=0.2 |\n",
    "            var sig;\n",
    "            sig = PinkNoise.ar(amp);\n",
    "            sig = BPF.ar(\n",
    "                sig,\n",
    "                freq.lag(lg),\n",
    "                rq.lag(lg),\n",
    "                // when a bandpass filter narrows, the amplitude decreases: this will balance it\n",
    "                1/rq.sqrt.lag(lg)\n",
    "            );\n",
    "            Out.ar(0, Pan2.ar(sig, pan));\n",
    "        }\"\"\"\n",
    "    )\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        self.amp = 0.3\n",
    "        self.base_tone = 69\n",
    "        self.log_mapping = False\n",
    "        \n",
    "        self.rx_bound = pi/2\n",
    "        self.ry_bound = pi/2\n",
    "        self.rz_bound = pi/4\n",
    "    \n",
    "    @bundle\n",
    "    def init_server(self):\n",
    "        self.synth_def.add()\n",
    "\n",
    "    @bundle\n",
    "    def start(self):\n",
    "        self.synth = scn.Synth(\"bpf_noise\", {\"amp\": 0, \"lg\": 0.015})\n",
    "    \n",
    "    @bundle\n",
    "    def _process(self, row):\n",
    "        \n",
    "        # use sin value as amplitude modulator\n",
    "        amp_mod = scn.linlin(row['value'], -1, 1, 0, 1, 'minmax')\n",
    "        \n",
    "        rx, ry, rz = row[['pose_Rx', 'pose_Ry', 'pose_Rz']]\n",
    "        \n",
    "        # use log() - 1 to map\n",
    "        if self.log_mapping:\n",
    "            rx_log = log2(scn.linlin(abs(rx), 0, self.rx_bound, 1, 2, 'minmax'))\n",
    "            rx_midi = scn.linlin(rx_log, 0, 1, 0, 12, 'minmax')\n",
    "            sign = 1 if rx >= 0 else -1\n",
    "            pitch = self.base_tone + sign * rx_midi\n",
    "            \n",
    "            ry_exp = scn.linlin(abs(ry), 0, self.ry_bound, 1, 2, 'minmax')\n",
    "            sign = 1 if ry >= 0 else -1\n",
    "            pan = sign * log2(ry_exp)\n",
    "                        \n",
    "            q =   scn.linlin(rz, -self.rz_bound, +self.rz_bound, 1, 100, 'minmax')\n",
    "            \n",
    "        else:\n",
    "            pitch = scn.linlin(rx, -self.rx_bound, +self.rx_bound, self.base_tone+12, self.base_tone-12, \"minmax\")\n",
    "            pan =   scn.linlin(ry, -self.ry_bound, +self.ry_bound, +1, -1, \"minmax\")\n",
    "            # linexp mapping to quality\n",
    "            q =   exp(scn.linlin(rz, -self.rz_bound, +self.rz_bound, log2(1), log2(100), 'minmax'))\n",
    "        \n",
    "        self.synth.set(\n",
    "            \"amp\", self.amp * amp_mod,\n",
    "            \"freq\", scn.midicps(pitch),\n",
    "            \"pan\", pan,\n",
    "            \"rq\", 1/q\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afb674",
   "metadata": {},
   "outputs": [],
   "source": [
    "son = NoisyHeadMulti()\n",
    "son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin = streams.DummySin('sin', kwargs={'fps': 500, 'timestamps': False}).test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f625af",
   "metadata": {},
   "source": [
    "Using **multi-threading** approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e3299",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rtdpmt = ps.RTDataPlayerMT([openface_stream, sin], son, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3037e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rtdpmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850934bb",
   "metadata": {},
   "source": [
    "Using **multi-processing** approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdpmp = ps.RTDataPlayerMP([openface_stream, sin], son, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e17f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rtdpmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:panson]",
   "language": "python",
   "name": "conda-env-panson-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
